Last login: Tue Oct  4 07:02:26 on console
localhost:~ NatarajanShankar$ curl https://slack.com/api/files.list?token=abcd -I
HTTP/1.1 200 OK
Content-Type: application/json; charset=utf-8
Connection: keep-alive
Access-Control-Allow-Origin: *
Content-Security-Policy: referrer no-referrer;
Date: Tue, 04 Oct 2016 16:17:44 GMT
Server: Apache
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
Vary: Accept-Encoding
X-Content-Type-Options: nosniff
X-Slack-Backend: h
X-Slack-Req-Id: 72db85b0-c132-4828-b3dd-2888d31b6c90
X-XSS-Protection: 0
X-Cache: Miss from cloudfront
Via: 1.1 f32e4aea3683be99c4324204c29f5852.cloudfront.net (CloudFront)
X-Amz-Cf-Id: VUfMKvM3KzmpQ9QZ9hSUiZ_hBg6Fjhd72hDes5szGLc6AbWxIYAnoQ==

localhost:~ NatarajanShankar$ !ssh
ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-54-227-71-81.compute-1.amazonaws.com
ssh: connect to host ec2-54-227-71-81.compute-1.amazonaws.com port 22: Operation timed out
localhost:~ NatarajanShankar$ !!
ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-54-227-71-81.compute-1.amazonaws.com
ssh: connect to host ec2-54-227-71-81.compute-1.amazonaws.com port 22: Operation timed out
localhost:~ NatarajanShankar$ ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-107-20-151-185.compute-1.amazonaws.com
The authenticity of host 'ec2-107-20-151-185.compute-1.amazonaws.com (107.20.151.185)' can't be established.
RSA key fingerprint is a8:5d:e8:8d:35:d6:cf:9c:d5:64:45:cd:8b:53:c0:a8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ec2-107-20-151-185.compute-1.amazonaws.com,107.20.151.185' (RSA) to the list of known hosts.
Last login: Tue Sep 27 22:34:43 2016 from 73.223.185.251
     ___   _        __   __   ____            __    
    / _ \ (_)___ _ / /  / /_ / __/____ ___ _ / /___ 
   / , _// // _ `// _ \/ __/_\ \ / __// _ `// // -_)
  /_/|_|/_/ \_, //_//_/\__//___/ \__/ \_,_//_/ \__/ 
           /___/                                                 
                                              
Welcome to a virtual machine image brought to you by RightScale!


[root@ip-172-31-6-204 ~]# show tables
-bash: show: command not found
[root@ip-172-31-6-204 ~]# hive
ls: cannot access /root/spark15/lib/spark-assembly-*.jar: No such file or directory

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.5.jar!/hive-log4j.properties
Exception in thread "main" java.lang.RuntimeException: java.net.ConnectException: Call From ip-172-31-6-204.ec2.internal/172.31.6.204 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:511)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Call From ip-172-31-6-204.ec2.internal/172.31.6.204 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:752)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1982)
	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1128)
	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1124)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1124)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1400)
	at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:587)
	at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:545)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:497)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 28 more
[root@ip-172-31-6-204 ~]# fdisk -l

Disk /dev/xvda1: 32.2 GB, 32212254720 bytes
255 heads, 63 sectors/track, 3916 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000


Disk /dev/xvdp: 107.4 GB, 107374182400 bytes
255 heads, 63 sectors/track, 13054 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

[root@ip-172-31-6-204 ~]# mount -t ext4 /dev/xvdp /data
[root@ip-172-31-6-204 ~]# /root/start-hadoop.sh
stopping datanode
Stopped Hadoop datanode:                                   [  OK  ]
starting datanode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-datanode-ip-172-31-6-204.out
Started Hadoop datanode (hadoop-hdfs-datanode):            [  OK  ]
no namenode to stop
Stopped Hadoop namenode:                                   [  OK  ]
starting namenode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-namenode-ip-172-31-6-204.out
Started Hadoop namenode:                                   [  OK  ]
stopping secondarynamenode
Stopped Hadoop secondarynamenode:                          [  OK  ]
starting secondarynamenode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-secondarynamenode-ip-172-31-6-204.out
Started Hadoop secondarynamenode:                          [  OK  ]
stopping resourcemanager
Stopped Hadoop resourcemanager:                            [  OK  ]
starting resourcemanager, logging to /var/log/hadoop-yarn/yarn-yarn-resourcemanager-ip-172-31-6-204.out
Started Hadoop resourcemanager:                            [  OK  ]
stopping nodemanager
Stopped Hadoop nodemanager:                                [  OK  ]
starting nodemanager, logging to /var/log/hadoop-yarn/yarn-yarn-nodemanager-ip-172-31-6-204.out
Started Hadoop nodemanager:                                [  OK  ]
stopping historyserver
Stopped Hadoop historyserver:                              [  OK  ]
starting historyserver, logging to /var/log/hadoop-mapreduce/mapred-mapred-historyserver-ip-172-31-6-204.out
16/10/04 18:54:47 INFO hs.JobHistoryServer: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting JobHistoryServer
STARTUP_MSG:   host = ip-172-31-6-204.ec2.internal/172.31.6.204
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0-cdh5.4.5
STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/aws-java-sdk-1.7.14.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/htrace-core-3.0.4.jar:/usr/lib/hadoop/lib/jasper-compiler-5.5.23.jar:/usr/lib/hadoop/lib/avro.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-el-1.0.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/jsch-0.1.42.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop/lib/zookeeper.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-collections-3.2.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/logredactor-1.0.3.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/httpclient-4.2.5.jar:/usr/lib/hadoop/lib/httpcore-4.2.5.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop/.//hadoop-common-tests.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//parquet-generator.jar:/usr/lib/hadoop/.//parquet-encoding.jar:/usr/lib/hadoop/.//hadoop-common-2.6.0-cdh5.4.5-tests.jar:/usr/lib/hadoop/.//parquet-thrift.jar:/usr/lib/hadoop/.//parquet-pig-bundle.jar:/usr/lib/hadoop/.//parquet-format-javadoc.jar:/usr/lib/hadoop/.//parquet-format.jar:/usr/lib/hadoop/.//parquet-column.jar:/usr/lib/hadoop/.//parquet-format-sources.jar:/usr/lib/hadoop/.//parquet-pig.jar:/usr/lib/hadoop/.//parquet-avro.jar:/usr/lib/hadoop/.//parquet-hadoop-bundle.jar:/usr/lib/hadoop/.//parquet-common.jar:/usr/lib/hadoop/.//hadoop-annotations-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop/.//hadoop-auth-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-aws-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//parquet-jackson.jar:/usr/lib/hadoop/.//parquet-hadoop.jar:/usr/lib/hadoop/.//parquet-cascading.jar:/usr/lib/hadoop/.//parquet-scrooge_2.10.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-nfs-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop/.//hadoop-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop/.//parquet-test-hadoop2.jar:/usr/lib/hadoop/.//parquet-protobuf.jar:/usr/lib/hadoop/.//parquet-scala_2.10.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//parquet-tools.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/htrace-core-3.0.4.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-el-1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jsp-api-2.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.6.0-cdh5.4.5-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/zookeeper.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jline-2.11.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/avro.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//mockito-all-1.8.5.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//microsoft-windowsazure-storage-sdk-0.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//htrace-core-3.0.4.jar:/usr/lib/hadoop-mapreduce/.//jasper-compiler-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//avro.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.4.5-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//commons-el-1.0.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.42.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//zookeeper.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.1.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//jasper-runtime-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//avro.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.1.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-el-1.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.4.5-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/.//htrace-core-3.0.4.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jasper-compiler-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//jasper-runtime-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.42.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//microsoft-windowsazure-storage-sdk-0.6.0.jar:/usr/lib/hadoop-mapreduce/.//mockito-all-1.8.5.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//zookeeper.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/avro.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/modules/*.jar
STARTUP_MSG:   build = http://github.com/cloudera/hadoop -r ab14c89fe25e9fb3f9de4fb852c21365b7c5608b; compiled by 'jenkins' on 2015-08-12T21:12Z
STARTUP_MSG:   java = 1.8.0_45
************************************************************/
Started Hadoop historyserver:                              [  OK  ]
[root@ip-172-31-6-204 ~]# /data/start_postgres.sh
could not change directory to "/root"
server starting
[root@ip-172-31-6-204 ~]# pwd
/root
[root@ip-172-31-6-204 ~]# /data/start_postgres.sh
could not change directory to "/root"
pg_ctl: another server might be running; trying to start server anyway
pg_ctl: could not start server
Examine the log output.
[root@ip-172-31-6-204 ~]# ls -la
total 124
dr-xr-x--- 13 root root  4096 Oct  4 18:52 .
dr-xr-xr-x 23 root root  4096 Oct  4 18:47 ..
-rw-------  1 root root 13326 Sep 28 05:11 .bash_history
-rw-r--r--  1 root root    25 Mar 17  2015 .bash_logout
-rw-r--r--  1 root root   199 Mar 17  2015 .bash_profile
-rw-r--r--  1 root root    75 Mar 17  2015 .bashrc
drwx------  4 root root  4096 May  4  2015 .cache
drwxr-xr-x  3 root root  4096 May  4  2015 .config
-rw-------  1 root root   287 Sep 22  2015 dans_bash_history.txt
-rw-rw-r--  1 root mail   135 Sep 21 16:31 dead.letter
-rw-r--r--  1 root root   633 Oct  4 18:52 derby.log
drwxr-xr-x  3 root root  4096 May  4  2015 .emacs.d
drwxr-xr-x 11 root root  4096 May  4  2015 ipython
drwxr-xr-x  4 root root  4096 May  4  2015 .ipython
drwxr-xr-x  5 root root  4096 Oct  4 18:52 metastore_db
drwxr-xr-x  5 root root  4096 May  4  2015 pgxl-deployment-tools
drwxr-----  3 root root  4096 May  4  2015 .pki
-rw-------  1 root root   391 Sep 28  2015 .psql_history
drwxr-xr-x  3 root root  4096 May  4  2015 .python-eggs
-rwxr-xr-x  1 root root  5716 Oct 10  2015 setup_ucb_complete_plus_postgres.sh
-rw-r--r--  1 root root    77 May  4  2015 .spark_history
drwx------  2 root root  4096 Sep 21 14:18 .ssh
-rwxr-xr-x  1 root root   221 Sep 22  2015 start-hadoop.sh
-rwxr-xr-x  1 root root   252 Sep 22  2015 start-hadoop.sh~
-rwxr-xr-x  1 root root   209 Sep 22  2015 stop-hadoop.sh
-rwxr-xr-x  1 root root   222 Sep 22  2015 stop-hadoop.sh~
drwxr-xr-x 11 root root  4096 May  4  2015 streamparse
[root@ip-172-31-6-204 ~]# su - w205
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3
Found 2 items
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3/user_data
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:55 /user/w205/lab_3/weblog_data
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3/user_data
Found 1 items
-rw-r--r--   1 w205 supergroup     166205 2016-09-24 19:54 /user/w205/lab_3/user_data/userdata_lab.csv
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3/weblog_data
Found 1 items
-rw-r--r--   1 w205 supergroup    5192992 2016-09-24 19:55 /user/w205/lab_3/weblog_data/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.5.jar!/hive-log4j.properties
WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> show tables
    > ;
OK
effective_care
hospitals
measures
readmissions
surveys_responses
user_info
weblogs_flat
weblogs_parquet
weblogs_schema
Time taken: 0.568 seconds, Fetched: 9 row(s)
hive> exit
    > ;
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ bash ./setup_spark.sh
[w205@ip-172-31-6-204 ~]$ /data/start_metastore.sh
[w205@ip-172-31-6-204 ~]$ SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Starting Hive Metastore Server
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

[w205@ip-172-31-6-204 ~]$ /data/spark15/bin/spark-sql
16/10/04 19:01:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/04 19:01:10 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
16/10/04 19:01:12 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
SET hive.support.sql11.reserved.keywords=false
16/10/04 19:01:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/04 19:01:15 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
SET spark.sql.hive.version=1.2.1
SET spark.sql.hive.version=1.2.1
spark-sql> 
         > 
         > SELECT user_id, COUNT(user_id) AS log_count
         > from weblogs_schema GROUP BY user_id
         > ORDER BY log_count DESC
         > LIMIT 50;
__RequestVerificationToken_Lw__=2C2DB   10                                      
__RequestVerificationToken_Lw__=32B2B	9
__RequestVerificationToken_Lw__=A1BC3	9
__RequestVerificationToken_Lw__=B2CC1	9
__RequestVerificationToken_Lw__=3BB1C	9
__RequestVerificationToken_Lw__=111AA	9
__RequestVerificationToken_Lw__=21DCC	9
__RequestVerificationToken_Lw__=D13BD	9
__RequestVerificationToken_Lw__=113B3	9
__RequestVerificationToken_Lw__=A223D	9
__RequestVerificationToken_Lw__=2BDC3	9
__RequestVerificationToken_Lw__=3DDC1	9
__RequestVerificationToken_Lw__=233C3	9
__RequestVerificationToken_Lw__=A31AB	9
__RequestVerificationToken_Lw__=2A231	8
__RequestVerificationToken_Lw__=C2221	8
__RequestVerificationToken_Lw__=1A2CA	8
__RequestVerificationToken_Lw__=11B3B	8
__RequestVerificationToken_Lw__=B2CCB	8
__RequestVerificationToken_Lw__=B32C2	8
__RequestVerificationToken_Lw__=CB1BC	8
__RequestVerificationToken_Lw__=1A2C1	8
__RequestVerificationToken_Lw__=1133C	8
__RequestVerificationToken_Lw__=A1D22	8
__RequestVerificationToken_Lw__=DAD1D	8
__RequestVerificationToken_Lw__=2CD1D	8
__RequestVerificationToken_Lw__=AAABA	8
__RequestVerificationToken_Lw__=CBCD3	8
__RequestVerificationToken_Lw__=DA1D2	8
__RequestVerificationToken_Lw__=3AA3C	8
__RequestVerificationToken_Lw__=CA22C	8
__RequestVerificationToken_Lw__=1221A	8
__RequestVerificationToken_Lw__=12CD1	8
__RequestVerificationToken_Lw__=AA1D3	8
__RequestVerificationToken_Lw__=B13AB	8
__RequestVerificationToken_Lw__=11DBC	8
__RequestVerificationToken_Lw__=B232C	8
__RequestVerificationToken_Lw__=D1DBD	8
__RequestVerificationToken_Lw__=BD11A	8
__RequestVerificationToken_Lw__=DBBC1	8
__RequestVerificationToken_Lw__=B2B32	8
__RequestVerificationToken_Lw__=33ABD	8
__RequestVerificationToken_Lw__=31A2B	8
__RequestVerificationToken_Lw__=CA3DD	8
__RequestVerificationToken_Lw__=A1ABB	8
__RequestVerificationToken_Lw__=2A2C1	8
__RequestVerificationToken_Lw__=B1ADC	8
__RequestVerificationToken_Lw__=C23DD	7
__RequestVerificationToken_Lw__=BC1DA	7
__RequestVerificationToken_Lw__=AC3DA	7
Time taken: 11.473 seconds, Fetched 50 row(s)
spark-sql> show log_count
         > ;
NoViableAltException(26@[731:1: ddlStatement : ( createDatabaseStatement | switchDatabaseStatement | dropDatabaseStatement | createTableStatement | dropTableStatement | truncateTableStatement | alterStatement | descStatement | showStatement | metastoreCheck | createViewStatement | dropViewStatement | createFunctionStatement | createMacroStatement | createIndexStatement | dropIndexStatement | dropFunctionStatement | reloadFunctionStatement | dropMacroStatement | analyzeStatement | lockStatement | unlockStatement | lockDatabase | unlockDatabase | createRoleStatement | dropRoleStatement | ( grantPrivileges )=> grantPrivileges | ( revokePrivileges )=> revokePrivileges | showGrants | showRoleGrants | showRolePrincipals | showRoles | grantRole | revokeRole | setRole | showCurrentRole );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2586)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1650)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/10/04 19:02:43 ERROR SparkSQLDriver: Failed in [show log_count
]
org.apache.spark.sql.AnalysisException: cannot recognize input near 'show' 'log_count' '<EOF>' in ddl statement; line 1 pos 5
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
org.apache.spark.sql.AnalysisException: cannot recognize input near 'show' 'log_count' '<EOF>' in ddl statement; line 1 pos 5
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

16/10/04 19:02:43 ERROR CliDriver: org.apache.spark.sql.AnalysisException: cannot recognize input near 'show' 'log_count' '<EOF>' in ddl statement; line 1 pos 5
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

spark-sql> CREATE TABLE weblogs_parquet AS SELECT * FROM weblogs_schema
         > ;
16/10/04 19:04:19 ERROR SparkSQLDriver: Failed in [CREATE TABLE weblogs_parquet AS SELECT * FROM weblogs_schema
]
org.apache.spark.sql.AnalysisException: default.weblogs_parquet already exists.;
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.run(CreateTableAsSelect.scala:86)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:144)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:129)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
org.apache.spark.sql.AnalysisException: default.weblogs_parquet already exists.;
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.run(CreateTableAsSelect.scala:86)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:144)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:129)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

16/10/04 19:04:19 ERROR CliDriver: org.apache.spark.sql.AnalysisException: default.weblogs_parquet already exists.;
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.run(CreateTableAsSelect.scala:86)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:144)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:129)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

spark-sql> SELECT user_id, COUNT(user_id) AS log_count
         > from weblogs_parquet GROUP BY user_id
         > ORDER BY log_count DESC
         > LIMIT 50;
__RequestVerificationToken_Lw__=2C2DB   10                                      
__RequestVerificationToken_Lw__=32B2B	9
__RequestVerificationToken_Lw__=A1BC3	9
__RequestVerificationToken_Lw__=B2CC1	9
__RequestVerificationToken_Lw__=111AA	9
__RequestVerificationToken_Lw__=3BB1C	9
__RequestVerificationToken_Lw__=3DDC1	9
__RequestVerificationToken_Lw__=113B3	9
__RequestVerificationToken_Lw__=21DCC	9
__RequestVerificationToken_Lw__=A223D	9
__RequestVerificationToken_Lw__=D13BD	9
__RequestVerificationToken_Lw__=2BDC3	9
__RequestVerificationToken_Lw__=233C3	9
__RequestVerificationToken_Lw__=A31AB	9
__RequestVerificationToken_Lw__=2A231	8
__RequestVerificationToken_Lw__=3AA3C	8
__RequestVerificationToken_Lw__=1A2CA	8
__RequestVerificationToken_Lw__=11B3B	8
__RequestVerificationToken_Lw__=31A2B	8
__RequestVerificationToken_Lw__=33ABD	8
__RequestVerificationToken_Lw__=1221A	8
__RequestVerificationToken_Lw__=1A2C1	8
__RequestVerificationToken_Lw__=1133C	8
__RequestVerificationToken_Lw__=A1D22	8
__RequestVerificationToken_Lw__=DAD1D	8
__RequestVerificationToken_Lw__=2CD1D	8
__RequestVerificationToken_Lw__=DBBC1	8
__RequestVerificationToken_Lw__=AAABA	8
__RequestVerificationToken_Lw__=B2CCB	8
__RequestVerificationToken_Lw__=DA1D2	8
__RequestVerificationToken_Lw__=C2221	8
__RequestVerificationToken_Lw__=CA3DD	8
__RequestVerificationToken_Lw__=12CD1	8
__RequestVerificationToken_Lw__=A1ABB	8
__RequestVerificationToken_Lw__=B13AB	8
__RequestVerificationToken_Lw__=11DBC	8
__RequestVerificationToken_Lw__=B232C	8
__RequestVerificationToken_Lw__=D1DBD	8
__RequestVerificationToken_Lw__=BD11A	8
__RequestVerificationToken_Lw__=B2B32	8
__RequestVerificationToken_Lw__=CBCD3	8
__RequestVerificationToken_Lw__=CA22C	8
__RequestVerificationToken_Lw__=B32C2	8
__RequestVerificationToken_Lw__=CB1BC	8
__RequestVerificationToken_Lw__=AA1D3	8
__RequestVerificationToken_Lw__=2A2C1	8
__RequestVerificationToken_Lw__=B1ADC	8
__RequestVerificationToken_Lw__=C23DD	7
__RequestVerificationToken_Lw__=D133C	7
__RequestVerificationToken_Lw__=BC1DA	7
Time taken: 3.646 seconds, Fetched 50 row(s)
spark-sql> get tables
         > ;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1071)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/10/04 19:24:07 ERROR SparkSQLDriver: Failed in [get tables
]
org.apache.spark.sql.AnalysisException: cannot recognize input near 'get' 'tables' '<EOF>'; line 1 pos 0
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
org.apache.spark.sql.AnalysisException: cannot recognize input near 'get' 'tables' '<EOF>'; line 1 pos 0
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

16/10/04 19:24:07 ERROR CliDriver: org.apache.spark.sql.AnalysisException: cannot recognize input near 'get' 'tables' '<EOF>'; line 1 pos 0
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

spark-sql> exit
         > ;
[w205@ip-172-31-6-204 ~]$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.5.jar!/hive-log4j.properties


WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> 
    > 
    > getr tables
    > ;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1025)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:394)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1111)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1159)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1048)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1038)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:0 cannot recognize input near 'getr' 'tables' '<EOF>'
hive> get tables;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1025)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:394)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1111)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1159)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1048)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1038)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:0 cannot recognize input near 'get' 'tables' '<EOF>'
hive> GET tables;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1025)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:394)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1111)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1159)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1048)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1038)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:0 cannot recognize input near 'GET' 'tables' '<EOF>'
hive> GET TABLES;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1025)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:394)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1111)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1159)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1048)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1038)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:0 cannot recognize input near 'GET' 'TABLES' '<EOF>'
hive> show tables;
OK
effective_care
hospitals
measures
readmissions
surveys_responses
user_info
weblogs_flat
weblogs_parquet
weblogs_schema
Time taken: 0.24 seconds, Fetched: 9 row(s)
hive> CREATE EXTERNAL TABLE IF NOT EXISTS hospitals
    > (hospitals string)
    > ROW FORMAT DELIMITED
    > STORED AS TEXTFILE
    > exit;
FAILED: ParseException line 5:0 extraneous input 'exit' expecting EOF near '<EOF>'
hive> exit;
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3/weblog_data
Found 1 items
-rw-r--r--   1 w205 supergroup    5192992 2016-09-24 19:55 /user/w205/lab_3/weblog_data/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3            
Found 2 items
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3/user_data
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:55 /user/w205/lab_3/weblog_data
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/     
Found 6 items
-rw-r--r--   1 w205 supergroup        693 2016-09-21 14:46 /user/w205/derby.log
-rw-r--r--   1 w205 supergroup       4103 2016-09-23 15:20 /user/w205/hive_base_ddl.sql
drwxr-xr-x   - w205 supergroup          0 2016-09-23 15:21 /user/w205/hospital_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-22 17:52 /user/w205/hospitals_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3
-rw-r--r--   1 w205 supergroup       1869 2016-09-22 17:58 /user/w205/load_data_lake.sh
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospital_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205                 
Found 6 items
-rw-r--r--   1 w205 supergroup        693 2016-09-21 14:46 /user/w205/derby.log
-rw-r--r--   1 w205 supergroup       4103 2016-09-23 15:20 /user/w205/hive_base_ddl.sql
drwxr-xr-x   - w205 supergroup          0 2016-09-23 15:21 /user/w205/hospital_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-22 17:52 /user/w205/hospitals_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3
-rw-r--r--   1 w205 supergroup       1869 2016-09-22 17:58 /user/w205/load_data_lake.sh
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospitals_compare
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ exit
logout
[root@ip-172-31-6-204 ~]# pwd
/root
[root@ip-172-31-6-204 ~]# ls -l
total 52
-rw-------  1 root root  287 Sep 22  2015 dans_bash_history.txt
-rw-rw-r--  1 root mail  135 Sep 21 16:31 dead.letter
-rw-r--r--  1 root root  633 Oct  4 18:52 derby.log
drwxr-xr-x 11 root root 4096 May  4  2015 ipython
drwxr-xr-x  5 root root 4096 Oct  4 18:52 metastore_db
drwxr-xr-x  5 root root 4096 May  4  2015 pgxl-deployment-tools
-rwxr-xr-x  1 root root 5716 Oct 10  2015 setup_ucb_complete_plus_postgres.sh
-rwxr-xr-x  1 root root  221 Sep 22  2015 start-hadoop.sh
-rwxr-xr-x  1 root root  252 Sep 22  2015 start-hadoop.sh~
-rwxr-xr-x  1 root root  209 Sep 22  2015 stop-hadoop.sh
-rwxr-xr-x  1 root root  222 Sep 22  2015 stop-hadoop.sh~
drwxr-xr-x 11 root root 4096 May  4  2015 streamparse
[root@ip-172-31-6-204 ~]# cd /data
[root@ip-172-31-6-204 data]# ls -la
total 68
drwxrwxrwx  8 root     root    4096 Sep 26 17:42 .
dr-xr-xr-x 23 root     root    4096 Oct  4 18:47 ..
drwxr-xr-x  3 hadoop   hadoop  4096 Sep 21 14:44 hadoop
drwxr-xr-x  3 hdfs     hdfs    4096 Sep 21 14:39 hadoop-hdfs
drwxr-xr-x  4 yarn     yarn    4096 Sep 21 14:39 hadoop-yarn
drwx------  2 root     root   16384 Sep 21 14:38 lost+found
drwxr-xr-x  4 postgres root    4096 Sep 21 14:43 pgsql
-rw-r--r--  1 root     root     535 Sep 21 14:44 setup_hive_for_postgres.sql
-rwxr-xr-x  1 root     root     732 Sep 21 14:44 setup_zeppelin.sh
drwxr-xr-x 11 w205     w205    4096 Sep 26 17:42 spark15
-rwxrwxr-x  1 w205     w205      27 Oct  4 18:59 start_metastore.sh
-rwxr-xr-x  1 root     root      93 Sep 21 14:44 start_postgres.sh
-rwxrwxr-x  1 w205     w205      94 Oct  4 18:59 stop_metastore.sh
-rwxr-xr-x  1 root     root      92 Sep 21 14:44 stop_postgres.sh
[root@ip-172-31-6-204 data]# cd ~
[root@ip-172-31-6-204 ~]# pwd
/root
[root@ip-172-31-6-204 ~]# whoami
root
[root@ip-172-31-6-204 ~]# su - w205
[w205@ip-172-31-6-204 ~]$ ls -l
total 279840
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospital_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/weblog*         
ls: `/user/w205/weblog*': No such file or directory
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/weblog_data
ls: `/user/w205/weblog_data': No such file or directory
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3      
Found 2 items
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3/user_data
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:55 /user/w205/lab_3/weblog_data
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospital_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospitals_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205                  
Found 6 items
-rw-r--r--   1 w205 supergroup        693 2016-09-21 14:46 /user/w205/derby.log
-rw-r--r--   1 w205 supergroup       4103 2016-09-23 15:20 /user/w205/hive_base_ddl.sql
drwxr-xr-x   - w205 supergroup          0 2016-09-23 15:21 /user/w205/hospital_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-22 17:52 /user/w205/hospitals_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3
-rw-r--r--   1 w205 supergroup       1869 2016-09-22 17:58 /user/w205/load_data_lake.sh
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ exit
logout
[root@ip-172-31-6-204 ~]# pwd
/root
[root@ip-172-31-6-204 ~]# ls -l
total 52
-rw-------  1 root root  287 Sep 22  2015 dans_bash_history.txt
-rw-rw-r--  1 root mail  135 Sep 21 16:31 dead.letter
-rw-r--r--  1 root root  633 Oct  4 18:52 derby.log
drwxr-xr-x 11 root root 4096 May  4  2015 ipython
drwxr-xr-x  5 root root 4096 Oct  4 18:52 metastore_db
drwxr-xr-x  5 root root 4096 May  4  2015 pgxl-deployment-tools
-rwxr-xr-x  1 root root 5716 Oct 10  2015 setup_ucb_complete_plus_postgres.sh
-rwxr-xr-x  1 root root  221 Sep 22  2015 start-hadoop.sh
-rwxr-xr-x  1 root root  252 Sep 22  2015 start-hadoop.sh~
-rwxr-xr-x  1 root root  209 Sep 22  2015 stop-hadoop.sh
-rwxr-xr-x  1 root root  222 Sep 22  2015 stop-hadoop.sh~
drwxr-xr-x 11 root root 4096 May  4  2015 streamparse
[root@ip-172-31-6-204 ~]# su - w205
[w205@ip-172-31-6-204 ~]$ ls -l
total 279840
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ whoami
w205
[w205@ip-172-31-6-204 ~]$ pwd  
/home/w205
[w205@ip-172-31-6-204 ~]$ ls -l
total 279840
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ ls -l
total 279840
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ ls -l
total 363356
-rw-r--r--  1 root root     13146 Oct  4 22:49 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  4 22:50 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  4 22:50 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  4 22:50 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ hdfs dfs -mkdir /user/w205/exercise_1/hospitals_compare
mkdir: `/user/w205/exercise_1/hospitals_compare': No such file or directory
[w205@ip-172-31-6-204 ~]$ hdfs dfs -mkdir /user/w205/exercise_1                  
[w205@ip-172-31-6-204 ~]$ hdfs dfs -mkdir /user/w205/exercise_1/hospitals_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -mkdir /user/w205/exercise_1/hospital_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -put *.csv /user/w205/exercise_1/hospital_compare/.
[w205@ip-172-31-6-204 ~]$ hdfs dfs -put *.csv /user/w205/exercise_1/hospitals_compare/.
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/exercise_1/hospitals_compare  
Found 7 items
-rw-r--r--   1 w205 supergroup      13146 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/Measures.csv
-rw-r--r--   1 w205 supergroup   63280769 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/effective_care.csv
-rw-r--r--   1 w205 supergroup     826758 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/hospitals.csv
-rw-r--r--   1 w205 supergroup   19936145 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/readmissions.csv
-rw-r--r--   1 w205 supergroup    1348499 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/surveys_responses.csv
-rw-r--r--   1 w205 supergroup     166205 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/userdata_lab.csv
-rw-r--r--   1 w205 supergroup    5192992 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/exercise_1/hospital_compare
Found 7 items
-rw-r--r--   1 w205 supergroup      13146 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/Measures.csv
-rw-r--r--   1 w205 supergroup   63280769 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/effective_care.csv
-rw-r--r--   1 w205 supergroup     826758 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/hospitals.csv
-rw-r--r--   1 w205 supergroup   19936145 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/readmissions.csv
-rw-r--r--   1 w205 supergroup    1348499 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/surveys_responses.csv
-rw-r--r--   1 w205 supergroup     166205 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/userdata_lab.csv
-rw-r--r--   1 w205 supergroup    5192992 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ vi /user/w205/exercise_1/hospital_compare/hospitals.csv
[w205@ip-172-31-6-204 ~]$ ls -l
total 363356
-rw-r--r--  1 root root     13146 Oct  4 22:49 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  4 22:50 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  4 22:50 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  4 22:50 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ vi hive_base_ddl.sql
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ !vi
vi hive_base_ddl.sql
[w205@ip-172-31-6-204 ~]$ ls -l
total 363356
-rw-r--r--  1 root root     13146 Oct  4 22:49 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  4 22:50 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  4 22:50 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  4 22:50 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ whoami
w205
[w205@ip-172-31-6-204 ~]$ chown w205 *.csv
chown: changing ownership of `Measures.csv': Operation not permitted
chown: changing ownership of `effective_care.csv': Operation not permitted
chown: changing ownership of `hospitals.csv': Operation not permitted
chown: changing ownership of `readmissions.csv': Operation not permitted
chown: changing ownership of `surveys_responses.csv': Operation not permitted
[w205@ip-172-31-6-204 ~]$ su
Password: 
su: incorrect password
[w205@ip-172-31-6-204 ~]$ su - root
Password: 
su: incorrect password
[w205@ip-172-31-6-204 ~]$ ls -l
total 363356
-rw-r--r--  1 root root     13146 Oct  4 22:49 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  4 22:50 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  4 22:50 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  4 22:50 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ ls -la
total 363400
drwx------  4 w205 w205      4096 Oct  4 23:10 .
drwxr-xr-x  7 root root      4096 Sep 22  2015 ..
-rw-------  1 w205 w205      5474 Oct  4 21:20 .bash_history
-rw-r--r--  1 w205 w205        18 Oct 16  2014 .bash_logout
-rw-r--r--  1 w205 w205       176 Oct 16  2014 .bash_profile
-rw-r--r--  1 w205 w205       124 Oct 16  2014 .bashrc
-rw-r--r--  1 w205 w205       500 May  7  2013 .emacs
-rw-------  1 w205 w205        16 Sep 28  2015 .psql_history
-rw-------  1 w205 w205      4271 Oct  4 23:10 .viminfo
-rw-r--r--  1 root root     13146 Oct  5 00:31 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  5 00:31 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  4 22:50 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  4 22:50 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ ls -la
total 363400
drwx------  4 w205 w205      4096 Oct  4 23:10 .
drwxr-xr-x  7 root root      4096 Sep 22  2015 ..
-rw-------  1 w205 w205      5474 Oct  4 21:20 .bash_history
-rw-r--r--  1 w205 w205        18 Oct 16  2014 .bash_logout
-rw-r--r--  1 w205 w205       176 Oct 16  2014 .bash_profile
-rw-r--r--  1 w205 w205       124 Oct 16  2014 .bashrc
-rw-r--r--  1 w205 w205       500 May  7  2013 .emacs
-rw-------  1 w205 w205        16 Sep 28  2015 .psql_history
-rw-------  1 w205 w205      4271 Oct  4 23:10 .viminfo
-rw-r--r--  1 root root     13146 Oct  5 00:31 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  5 00:32 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  5 00:32 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  5 00:32 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ ls -l *.csv
-rw-r--r-- 1 root root    13146 Oct  5 00:31 Measures.csv
-rw-r--r-- 1 root root 63280769 Oct  5 00:32 effective_care.csv
-rw-r--r-- 1 root root   826758 Oct  5 00:32 hospitals.csv
-rw-r--r-- 1 root root 19936145 Oct  5 00:32 readmissions.csv
-rw-r--r-- 1 root root  1348499 Oct  5 00:32 surveys_responses.csv
-rw-rw-r-- 1 w205 w205   166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r-- 1 w205 w205  5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ rm hospitals.csv 
rm: remove write-protected regular file `hospitals.csv'? yes
[w205@ip-172-31-6-204 ~]$ rm effective_care.csv 
rm: remove write-protected regular file `effective_care.csv'? yes
[w205@ip-172-31-6-204 ~]$ rm Measures.csv 
rm: remove write-protected regular file `Measures.csv'? yes
[w205@ip-172-31-6-204 ~]$ rm readmissions.csv 
rm: remove write-protected regular file `readmissions.csv'? yes
[w205@ip-172-31-6-204 ~]$ ls -l
total 281164
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  5 00:32 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ rm surveys_responses.csv 
rm: remove write-protected regular file `surveys_responses.csv'? yes
[w205@ip-172-31-6-204 ~]$ vi hive_base_ddl.sql 

[No write since last change]
/bin/bash: wq: command not found

shell returned 127

Press ENTER or type command to continue
[No write since last change]
 00:35:13 up  5:47,  2 users,  load average: 0.00, 0.00, 0.00
USER     TTY      FROM              LOGIN@   IDLE   JCPU   PCPU WHAT
root     pts/0    73.223.185.251   18:52    0.00s 22.94s  0.02s vim hive_base_ddl.sql
root     pts/1    73.223.185.251   23:10    1:24m  0.02s  0.02s vim hive_base_ddl.sql

Press ENTER or type command to continue
[w205@ip-172-31-6-204 ~]$ !vi
vi hive_base_ddl.sql 
[w205@ip-172-31-6-204 ~]$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.5.jar!/hive-log4j.properties
WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> DROP TABLE IF EXISTS hospitals;
OK
Time taken: 0.903 seconds
hive> CREATE EXTERNAL TABLE   hospitals(
    >                 Provider_ID             string,
    >                 Name                    string,
    >                 Address                 string,
    >                 City                    string,
    >                 State                   string,
    >                 ZIP_CODE                string,
    >                 County                  string,
    >                 Phone_Number            string,
    >                 Hospital_Type           string,
    >                 Ownership               string,
    >                 Emergency_Services      string
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
OK
Time taken: 0.192 seconds
hive> DROP TABLE IF EXISTS effective_care;
OK
Time taken: 0.106 seconds
hive> CREATE EXTERNAL TABLE effective_care(
    >                 Provider_ID             string,
    >                 Name                    string,
    >                 Address                 string,
    >                 City                    string,
    >                 State                   string,
    >                 ZIP_CODE                string,
    >                 County                  string,
    >                 Phone_Number            string,
    >                 Measure_ID              string,
    >                 Measure_Name            string,
    >                 Score                   int,
    >                 Sample                  int,
    >                 Footnote                string,
    >                 Start_Date              date,
    >                 End_Date                date
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
OK
Time taken: 0.073 seconds
hive> DROP TABLE IF EXISTS readmissions;
OK
Time taken: 0.103 seconds
hive> CREATE EXTERNAL TABLE readmissions(
    >                 Provider_ID             string,
    >                 Name                    string,
    >                 Address                 string,
    >                 City                    string,
    >                 State                   string,
    >                 ZIP_CODE                string,
    >                 County                  string,
    >                 Phone_Number            string,
    >                 Measure_ID              string,
    >                 Measure_Name            string,
    >                 Compared_to_National    string,
    >                 Denominator             string,
    >                 Score                   int,
    >                 Lower_Estimate          int,
    >                 Higher_Estimate         int,
    >                 Footnote                string,
    >                 Start_Date              date,
    >                 End_Date                date
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\' )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
OK
Time taken: 0.06 seconds
hive> DROP TABLE IF EXISTS measures;
OK
Time taken: 0.085 seconds
hive> CREATE EXTERNAL TABLE measures(
    >                 Measure_Name            string,
    >                 Measure_ID              string,
    >                 Measure_StartQuarter    string,
    >                 Measure_Start_Date      datetime,
    >                 Measure_EndQuarter      string,
    >                 Measure_End_Date        datetime
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > exit;
FAILED: ParseException line 16:0 extraneous input 'exit' expecting EOF near '<EOF>'
hive> DROP TABLE IF EXISTS measures;
OK
Time taken: 0.02 seconds
hive> CREATE EXTERNAL TABLE measures(
    >                 Measure_Name            string,
    >                 Measure_ID              string,
    >                 Measure_StartQuarter    string,
    >                 Measure_Start_Date      datetime,
    >                 Measure_End_Quarter     string,
    >                 Measure_End_Date        datetime
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
FAILED: SemanticException [Error 10099]: DATETIME type isn't supported yet. Please use DATE or TIMESTAMP instead
hive> DROP TABLE IF EXISTS measures;
OK
Time taken: 0.011 seconds
hive> CREATE EXTERNAL TABLE measures(
    >                 Measure_Name            string,
    >                 Measure_ID              string,
    >                 Measure_StartQuarter    string,
    >                 Measure_Start_Date      timestamp,
    >                 Measure_End_Quarter     string,
    >                 Measure_End_Date        datetime
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
FAILED: SemanticException [Error 10099]: DATETIME type isn't supported yet. Please use DATE or TIMESTAMP instead
hive> DROP TABLE IF EXISTS measures;
OK
Time taken: 0.009 seconds
hive> CREATE EXTERNAL TABLE measures(
    >                 Measure_Name            string,
    >                 Measure_ID              string,
    >                 Measure_StartQuarter    string,
    >                 Measure_Start_Date      timestamp,
    >                 Measure_End_Quarter     string,
    >                 Measure_End_Date        timestamp
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
OK
Time taken: 0.055 seconds
hive> DROP TABLE IF EXISTS surveys_responses;
OK
Time taken: 0.119 seconds
hive> CREATE EXTERNAL TABLE surveys_responses(
    >                 Provider_ID             string,
    >                 Name                    string,
    >                 Address                 string,
    >                 City                    string,
    >                 State                   string,
    >                 ZIP_CODE                string,
    >                 County                  string,
    >                 Communication_with_Nurses_Achievement_Points    string,
    >                 Communication_with_Nurses_Improvement_Points    string,
    >                 Communication_with_Nurses_Dimension_Score       string,
    >                 Communication_with_Doctors_Achievement_Points   string,
    >                 Communication_with_Doctors_Improvement_Points   string,
    >                 Communication_with_Doctors_Dimension_Score      string,
    >                 Responsiveness_of_Hospital_Staff_Achievement_Points     string,
    >                 Responsiveness_of_Hospital_Staff_Improvement_Points     string,
    >                 Responsiveness_of_Hospital_Staff_Dimension_Score        string,
    >                 Pain_Management_Achievement_Points              string,
    >                 Pain_Management_Improvement_Points              string,
    >                 Pain_Management_Dimension_Score                 string,
    >                 Communication_about_Medicines_Achievement_Points        string,
    >                 Communication_about_Medicines_Improvement_Points        string,
    >                 Communication_about_Medicines_Dimension_Score           string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Achievement_Points    string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Improvement_Points    string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Dimension_Score       string,
    >                 Discharge_Information_Achievement_Points        string,
    >                 Discharge_Information_Improvement_Points        string,
    >                 Discharge_Information_Dimension_Score           string,
    >                 Overall_Rating_of_Hospital_Achievement_Points   string,
    >                 Overall_Rating_of_Hospital_Improvement_Points   string,
    >                 Overall_Rating_of_Hospital_Dimension_Score      string,
    >                 HCAHPS_Base_Score                               INT,
    >                 HCAHPS_Consistency_Score                        INT
    > )
    > STORED AS TEXTFILE
    > 
    > LOCSATION '/user/w205/hospital_compare/';
FAILED: ParseException line 38:0 missing EOF at 'LOCSATION' near 'TEXTFILE'
hive> DROP TABLE IF EXISTS surveys_responses;
OK
Time taken: 0.009 seconds
hive> CREATE EXTERNAL TABLE surveys_responses(
    >                 Provider_ID             string,
    >                 Name                    string,
    >                 Address                 string,
    >                 City                    string,
    >                 State                   string,
    >                 ZIP_CODE                string,
    >                 County                  string,
    >                 Communication_with_Nurses_Achievement_Points    string,
    >                 Communication_with_Nurses_Improvement_Points    string,
    >                 Communication_with_Nurses_Dimension_Score       string,
    >                 Communication_with_Doctors_Achievement_Points   string,
    >                 Communication_with_Doctors_Improvement_Points   string,
    >                 Communication_with_Doctors_Dimension_Score      string,
    >                 Responsiveness_of_Hospital_Staff_Achievement_Points     string,
    >                 Responsiveness_of_Hospital_Staff_Improvement_Points     string,
    >                 Responsiveness_of_Hospital_Staff_Dimension_Score        string,
    >                 Pain_Management_Achievement_Points              string,
    >                 Pain_Management_Improvement_Points              string,
    >                 Pain_Management_Dimension_Score                 string,
    >                 Communication_about_Medicines_Achievement_Points        string,
    >                 Communication_about_Medicines_Improvement_Points        string,
    >                 Communication_about_Medicines_Dimension_Score           string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Achievement_Points    string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Improvement_Points    string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Dimension_Score       string,
    >                 Discharge_Information_Achievement_Points        string,
    >                 Discharge_Information_Improvement_Points        string,
    >                 Discharge_Information_Dimension_Score           string,
    >                 Overall_Rating_of_Hospital_Achievement_Points   string,
    >                 Overall_Rating_of_Hospital_Improvement_Points   string,
    >                 Overall_Rating_of_Hospital_Dimension_Score      string,
    >                 HCAHPS_Base_Score                               INT,
    >                 HCAHPS_Consistency_Score                        INT
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
OK
Time taken: 0.106 seconds
hive> SELECT Provider_ID, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals 
    > ORDER BY hospital_count
    > LIMIT 100;
FAILED: SemanticException [Error 10025]: Line 1:7 Expression not in GROUP BY key 'Provider_ID'
hive> SELECT Provider_ID, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals 
    > GROUP BY Provider_ID
    > ORDER BY hospital_count
    > LIMIT 100;
Query ID = w205_20161005005454_1b1da24f-d111-47a6-beff-b6f98eeeb6e0
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0001, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0001/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-10-05 00:54:31,655 Stage-1 map = 0%,  reduce = 0%
2016-10-05 00:54:38,280 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.19 sec
2016-10-05 00:54:44,663 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.85 sec
MapReduce Total cumulative CPU time: 2 seconds 850 msec
Ended Job = job_1475607266406_0001
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0002, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0002/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0002
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2016-10-05 00:54:55,210 Stage-2 map = 0%,  reduce = 0%
2016-10-05 00:55:01,736 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.53 sec
2016-10-05 00:55:08,080 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.97 sec
MapReduce Total cumulative CPU time: 2 seconds 970 msec
Ended Job = job_1475607266406_0002
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.85 sec   HDFS Read: 7464 HDFS Write: 96 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.97 sec   HDFS Read: 4548 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 820 msec
OK
Time taken: 50.151 seconds
hive> SELECT Provider_ID, COUNT(Provider_ID) AS hospital_count
    > GROUP BY Provider_ID
    > LIMIT 100;
FAILED: SemanticException [Error 10004]: Line 2:9 Invalid table alias or column reference 'Provider_ID': (possible column names are: )
hive> SELECT Provider_ID FROM hospitals;
OK
Time taken: 0.057 seconds
hive> SELECT Provider_ID FROM hospitals
    > LIMIT 100;
OK
Time taken: 0.051 seconds
hive> SELECT * FROM hospitals
    > LIMIT 100;
OK
Time taken: 0.075 seconds
hive> exit
    > ;
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ hdfs dfs -l /user/w205/hospital_compare
-l: Unknown command
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospital_compare
[w205@ip-172-31-6-204 ~]$ history               
    1  wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.0-bin-hadoop2.6.tgz
    2  tar xvzf spark-1.5.0-bin-hadoop2.6.tgz
    3  mv spark-1.5.0-bin-hadoop2.6 spark15
    4  $SPARK_HOME/bin/pyspark --master yarn
    5  exit
    6  export HIVE_CONF_DIR=/data/hadoop/hive/conf
    7  hive
    8  exit
    9  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   10  hive
   11  psql -U hiveuser -d metastore
   12  ifconfig
   13  exit
   14  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   15  hive
   16  exit
   17  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   18  hive
   19  exit
   20  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   21  hive –e “s'ow tables;?
   22  exit
   23  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   24  hive --help
   25  hive --service
   26  exit
   27  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   28  hive
   29  exit
   30  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   31  hive
   32  exit
   33  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   34  hive
   35  exit
   36  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   37  hive
   38  exit
   39  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   40  hive
   41  exit;
   42  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   43  hive
   44  exit
   45  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   46  hive
   47  psql -h localhost -U hiveuser -d metastore
   48  exit
   49  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   50  hive
   51  exit;
   52  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   53  hive
   54  hive --service metastore
   55  service hive-metastore start
   56  ls /etc/init.d/*hive*
   57  exit
   58  hive
   59  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   60  hive
   61  exit
   62  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   63  hive -e 'show tables;'
   64  exit
   65  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   66  /home/w205/spark15/bin/spark-sql 
   67  exit;
   68  psql -d w205
   69  exit;
   70  pwd
   71  ls -alF
   72  ls -la
   73  hdfs dfs -ls /user
   74  hdfs dfs -put derby.log /user/w205
   75  hdfs dfs -ls /user/w205
   76  hdfs dfs ls /user
   77  hdfs dfs -ls /user
   78  pwd
   79  hdfs dfs -mkdir /user/w205/hospital_compare
   80  hdfs dfs -ls /user/w205
   81  whoami
   82  pwd
   83  ls -la
   84  pwd
   85  cd /user/w205
   86  pwd
   87  whoami
   88  pwd
   89  whamil
   90  whoami
   91  exit
   92  pwd
   93  exit
   94  hdfs dfs -put /root/.csv /user/w205/.
   95  hdfs dfs -put /root/*.csv /user/w205/.
   96  cd /root
   97  exit
   98  pwd
   99  cd /user/w205
  100  hdfs dfs -l /user
  101  hdfs dfs -ls /user
  102  cd /home
  103  ls -l
  104  cd w205
  105  whoami
  106  ls -l
  107  ls -la
  108  pwd
  109  ls -la
  110  tie
  111  time
  112  clock
  113  date
  114  sudo tzselect
  115  pwd
  116  ls -l
  117  hdfs dfs -ls /user/w205
  118  hdfs dfs -ls /user/w205/hosp*
  119  hdfs dfs -put *.csv /user/w205/hospital_compare/.
  120  hdfs dfs -ls /user/w205/hosp*
  121  ped
  122  pwd
  123  ls -l
  124  ls-la
  125  ls -la
  126  hdfd dfs -ls /home/w205/hosp*
  127  hdfs dfs -ls /home/w205/hosp*
  128  hdfs dfs -ls /user/w205/hosp*
  129  exit
  130  pwd
  131  ls -l
  132  cd /user/w205
  133  exit
  134  hdfs dfs -rm /user/w205/hospital_compare/*
  135  hdfs dfs -ls /user/w205/hospital_compare
  136  hdfs dfs -rmdir /user/w205/hospital_compare
  137  hdfs dfs -mkdir /user/w205/hospitals_compare
  138  hdfs dfs -ls /user/w205/
  139  exit
  140  hdfs dfs -put load_data_lake.sh /user/w205
  141  hdfs dfs -ls /user/w205
  142  pwd
  143  ls -l
  144  hdfs dfs put hive_base_ddl.sql /user/w205
  145  hdfs dfs -put hive_base_ddl.sql /user/w205
  146  cd /user/w205
  147  pwd
  148  hive -f ./hive_base_ddl.sql
  149  ls -l
  150  [w205@ip-172-31-6-204 ~]$ hive -f ./hive_base_ddl.sql
  151  SLF4J: Class path contains multiple SLF4J bindings.
  152  SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
  153  SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
  154  SLF4J: Class path contains multiple SLF4J bindings.
  155  SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
  156  SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
  157  OK
  158  Time taken: 0.463 seconds
  159  OK
  160  Time taken: 0.332 seconds
  161  OK
  162  Time taken: 0.01 seconds
  163  OK
  164  Time taken: 0.053 seconds
  165  OK
  166  Time taken: 0.009 seconds
  167  OK
  168  Time taken: 0.069 seconds
  169  OK
  170  Time taken: 0.012 seconds
  171  OK
  172  Time taken: 0.042 seconds
  173  OK
  174  Time taken: 0.009 seconds
  175  OK
  176  Time taken: 0.093 seconds
  177  hdfs dfs -mkdir /user/w205/lab_3
  178  wget https://s3.amazonaws.com/ucbdatasciencew205/lab_datasets/userdata_lab.csv
  179  wget https://s3.amazonaws.com/ucbdatasciencew205/lab_datasets/weblog_lab.csv
  180  ls -la
  181  hdfs dfs -mkdir /user/w205/lab_3/user_data
  182  hdfs dfs -mkdir /user/w205/lab_3/weblog_data
  183  hdfs dfs -put userdata_lab.csv /user/w205/lab_3/user_data
  184  hdfs dfs -put weblog_lab.csv /user/w205/lab_3/weblog_data
  185  ls -la
  186  cat userdata_lab.csv 
  187  cat weblog_lab.csv 
  188  hive
  189  whoami
  190  hdfs dfs -ls -la /user/w205/lab_3
  191  hdfs dfs -ls /user/w205/lab_3
  192  hdfs dfs -ls /user/w205/lab_3/user_data
  193  hdfs dfs -ls /user/w205/lab_3/weblog_data
  194  quit
  195  exit
  196  hive
  197  exit
  198  hive
  199  pwd
  200  SPARK SQL CLI
  201  wget https://s3.awazonaws.com/ucbdatasciencew205/setup_spark.sh
  202  wget https://s3.amazonaws.com/ucbdatasciencew205/setup_spark.sh
  203  ls -la
  204  vi setup_spark.sh
  205  bash ./setup_spark.sh
  206  vi setup_spark.sh
  207  ls -l /data
  208  cat start_metastore.sh
  209  cat /data/start_metastore.sh
  210  /data/start_metastore.sh
  211  /data/spark15/bin/spark-sql
  212  pwd
  213  ls -la
  214  cd spark15
  215  ls -l
  216  cd conf
  217  ls -l
  218  vi spark-defaults.conf.template
  219  fgrep rootCategory *
  220  vi log4j.properties.template
  221  pwd
  222  cd
  223  pwd
  224  ls -l
  225  history
  226  sc.setLogLevel("ERROR")
  227  sc.setLogLevel("INFO")
  228  /data/spark15/bin/spark-sql
  229  spark-shell
  230  export PATH = $PATH:/usr/local/spark/bin
  231  ls -l
  232  ls -l spark15
  233  cat README.md
  234  cd spark15
  235  cat README.md
  236  ls -la
  237  cd conf
  238  ls -l *prop*
  239  vi log4j.properties.template
  240  exit
  241  hdfs dfs -ls /user/w205/lab_3
  242  hdfs dfs -ls /user/w205/lab_3/user_data
  243  hdfs dfs -ls /user/w205/lab_3/weblog_data
  244  hive
  245  pwd
  246  bash ./setup_spark.sh
  247  /data/start_metastore.sh
  248  /data/spark15/bin/spark-sql
  249  hive
  250  pwd
  251  hdfs dfs -ls /user/w205/lab_3/weblog_data
  252  hdfs dfs -ls /user/w205/lab_3
  253  hdfs dfs -ls /user/w205/
  254  hdfs dfs -ls /user/w205/hospital_compare
  255  hdfs dfs -ls /user/w205
  256  hdfs dfs -ls /user/w205/hospitals_compare
  257  pwd
  258  exit
  259  pwd
  260  ls -l
  261  exir
  262  exit
  263  ls -l
  264  hdfs dfs -ls /user/w205/hospital_compare
  265  hdfs dfs -ls /user/w205/weblog*
  266  hdfs dfs -ls /user/w205/weblog_data
  267  hdfs dfs -ls /user/w205/lab_3
  268  hdfs dfs -ls /user/w205/hospital_compare
  269  hdfs dfs -ls /user/w205/hospitals_compare
  270  hdfs dfs -ls /user/w205
  271  pwd
  272  exit
  273  ls -l
  274  pwd
  275  whoami
  276  pwd
  277  ls -l
  278  pwd
  279  ls -l
  280  pwd
  281  hdfs dfs -mkdir /user/w205/exercise_1/hospitals_compare
  282  hdfs dfs -mkdir /user/w205/exercise_1
  283  hdfs dfs -mkdir /user/w205/exercise_1/hospitals_compare
  284  hdfs dfs -mkdir /user/w205/exercise_1/hospital_compare
  285  hdfs dfs -put *.csv /user/w205/exercise_1/hospital_compare/.
  286  hdfs dfs -put *.csv /user/w205/exercise_1/hospitals_compare/.
  287  hdfs dfs -ls /user/w205/exercise_1/hospitals_compare
  288  hdfs dfs -ls /user/w205/exercise_1/hospital_compare
  289  vi /user/w205/exercise_1/hospital_compare/hospitals.csv
  290  ls -l
  291  pwd
  292  vi hive_base_ddl.sql
  293  ls -l
  294  whoami
  295  chown w205 *.csv
  296  su
  297  su - root
  298  ls -l
  299  ls -la
  300  ls -l *.csv
  301  rm hospitals.csv 
  302  rm effective_care.csv 
  303  rm Measures.csv 
  304  rm readmissions.csv 
  305  ls -l
  306  rm surveys_responses.csv 
  307  vi hive_base_ddl.sql 
  308  hive
  309  pwd
  310  hdfs dfs -l /user/w205/hospital_compare
  311  hdfs dfs -ls /user/w205/hospital_compare
  312  history
[w205@ip-172-31-6-204 ~]$ ls -la
total 279904
drwx------  4 w205 w205      4096 Oct  5 00:49 .
drwxr-xr-x  7 root root      4096 Sep 22  2015 ..
-rw-------  1 w205 w205      5474 Oct  4 21:20 .bash_history
-rw-r--r--  1 w205 w205        18 Oct 16  2014 .bash_logout
-rw-r--r--  1 w205 w205       176 Oct 16  2014 .bash_profile
-rw-r--r--  1 w205 w205       124 Oct 16  2014 .bashrc
-rw-r--r--  1 w205 w205       500 May  7  2013 .emacs
-rw-r--r--  1 w205 w205     20480 Oct  5 00:49 .hive_base_ddl.sql.swp
-rw-------  1 w205 w205        16 Sep 28  2015 .psql_history
-rw-------  1 w205 w205      4614 Oct  5 00:44 .viminfo
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 w205 w205      4108 Oct  5 00:49 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/exercise_1/hospital_compare
Found 7 items
-rw-r--r--   1 w205 supergroup      13146 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/Measures.csv
-rw-r--r--   1 w205 supergroup   63280769 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/effective_care.csv
-rw-r--r--   1 w205 supergroup     826758 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/hospitals.csv
-rw-r--r--   1 w205 supergroup   19936145 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/readmissions.csv
-rw-r--r--   1 w205 supergroup    1348499 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/surveys_responses.csv
-rw-r--r--   1 w205 supergroup     166205 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/userdata_lab.csv
-rw-r--r--   1 w205 supergroup    5192992 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ ls -la
total 363420
drwx------  4 w205 w205      4096 Oct  5 01:01 .
drwxr-xr-x  7 root root      4096 Sep 22  2015 ..
-rw-------  1 w205 w205      5474 Oct  4 21:20 .bash_history
-rw-r--r--  1 w205 w205        18 Oct 16  2014 .bash_logout
-rw-r--r--  1 w205 w205       176 Oct 16  2014 .bash_profile
-rw-r--r--  1 w205 w205       124 Oct 16  2014 .bashrc
-rw-r--r--  1 w205 w205       500 May  7  2013 .emacs
-rw-r--r--  1 w205 w205     20480 Oct  5 00:49 .hive_base_ddl.sql.swp
-rw-------  1 w205 w205        16 Sep 28  2015 .psql_history
-rw-------  1 w205 w205      4614 Oct  5 00:44 .viminfo
-rw-r--r--  1 root root     13146 Oct  5 01:00 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  5 01:01 effective_care.csv
-rw-r--r--  1 w205 w205      4108 Oct  5 00:49 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  5 01:01 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  5 01:01 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  5 01:01 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hdfs dfs -put *.csv /user/w205/hospital_compare/.
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospital_compare
Found 7 items
-rw-r--r--   1 w205 supergroup      13146 2016-10-05 01:03 /user/w205/hospital_compare/Measures.csv
-rw-r--r--   1 w205 supergroup   63280769 2016-10-05 01:03 /user/w205/hospital_compare/effective_care.csv
-rw-r--r--   1 w205 supergroup     826758 2016-10-05 01:03 /user/w205/hospital_compare/hospitals.csv
-rw-r--r--   1 w205 supergroup   19936145 2016-10-05 01:03 /user/w205/hospital_compare/readmissions.csv
-rw-r--r--   1 w205 supergroup    1348499 2016-10-05 01:03 /user/w205/hospital_compare/surveys_responses.csv
-rw-r--r--   1 w205 supergroup     166205 2016-10-05 01:03 /user/w205/hospital_compare/userdata_lab.csv
-rw-r--r--   1 w205 supergroup    5192992 2016-10-05 01:03 /user/w205/hospital_compare/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.5.jar!/hive-log4j.properties
WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> SELECT * FROM hospitals
    > LIMIT 10;
OK
ACS Participation data	ACS_REGISTRY	3Q2013	2013-07-01 00:00:00	2Q2014	2014-06-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Statin Prescribed at Discharge	AMI_10	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Aspirin Prescribed at Discharge	AMI_2	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Fibrinolytic Therapy Received within 30 Minutes of Hospital Arrival	AMI_7a	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Primary PCI Received Within 90 Minutes of Hospital Arrival	AMI_8a	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Relievers for Inpatient Asthma	CAC_1	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Systemic Corticosteroids for Inpatient Asthma	CAC_2	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Home Management Plan of Care (HMPC) Document Given to Patient/Caregiver	CAC_3	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Complication Rate Following Elective Primary  Total Hip Arthroplasty (THA) and/or Total Knee Arthroplasty (TKA)	COMP_HIP_KNEE	2Q2011	2011-04-01 00:00:00	1Q2014	2014-03-31 00:00:00	NULL	NULL	NULL	NULL	NULL
Median Time from ED Arrival to ED Departure for Admitted ED Patients	ED_1b	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Time taken: 0.921 seconds, Fetched: 10 row(s)
hive> SELECT Provider_ID FROM hospitals, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 100;
FAILED: ParseException line 1:40 missing EOF at '(' near 'COUNT'
hive> SELECT Provider_ID COUNT(Provider_ID) AS hospital_count
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 100;
FAILED: ParseException line 1:24 missing EOF at '(' near 'COUNT'
hive> SELECT Provider_ID, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 100;
Query ID = w205_20161005010808_ffef174d-6f8d-40c6-953f-dac650664b1b
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0003, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0003/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-10-05 01:08:58,022 Stage-1 map = 0%,  reduce = 0%
2016-10-05 01:09:10,113 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.86 sec
2016-10-05 01:09:16,619 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.52 sec
MapReduce Total cumulative CPU time: 10 seconds 520 msec
Ended Job = job_1475607266406_0003
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 10.52 sec   HDFS Read: 90773136 HDFS Write: 1000 SUCCESS
Total MapReduce CPU Time Spent: 10 seconds 520 msec
OK
010001	64
010005	64
010006	64
010007	64
010008	64
010011	64
010012	64
010016	64
010018	63
010019	64
010021	64
010022	64
010023	64
010024	64
010029	64
010032	64
010033	64
010034	64
010035	64
010036	64
010038	64
010039	67
010040	64
010044	64
010045	64
010046	64
010047	64
010049	64
010051	63
010052	64
010054	64
010055	64
010056	64
010058	63
010059	64
010061	64
010062	64
010065	64
010069	64
010073	64
010078	64
010079	64
010083	64
010085	64
010086	64
010087	64
010089	64
010090	64
010091	64
010092	64
010095	63
010097	64
010099	64
010100	64
010101	64
010102	63
010103	64
010104	64
010108	64
010109	64
010110	63
010112	64
010113	64
010114	64
010118	64
010120	64
010125	64
010126	64
010128	63
010129	64
010130	64
010131	64
010138	63
010139	64
010144	64
010146	64
010148	64
010149	64
01014F	27
010150	64
010157	64
010158	64
010164	64
010168	64
010169	64
01019F	27
01021F	27
011300	63
011302	63
011304	62
013301	54
020001	64
020006	64
020008	64
020012	67
020017	64
020018	52
020024	64
020026	64
020027	51
Time taken: 30.031 seconds, Fetched: 100 row(s)
hive> SELECT Provider_ID, Name, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 100;
FAILED: SemanticException [Error 10025]: Line 1:20 Expression not in GROUP BY key 'Name'
hive> SELECT Provider_ID, Name, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals
    > LIMIT 100;
FAILED: SemanticException [Error 10025]: Line 1:7 Expression not in GROUP BY key 'Provider_ID'
hive> SELECT Provider_ID, Name, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 100;
FAILED: SemanticException [Error 10025]: Line 1:20 Expression not in GROUP BY key 'Name'
hive> SELECT Provider_ID, Name, COUNT(Provider_ID) AS hospital_count
    > LIMIT 10;
FAILED: SemanticException [Error 10004]: Line 1:32 Invalid table alias or column reference 'Provider_ID': (possible column names are: )
hive> SELECT Provider_ID, Name
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 10;
FAILED: SemanticException [Error 10025]: Line 1:20 Expression not in GROUP BY key 'Name'
hive> SELECT Provider_ID, Name
    > FROM hospitals
    > LIMIT 100;
OK
ACS Participation data	ACS_REGISTRY
Statin Prescribed at Discharge	AMI_10
Aspirin Prescribed at Discharge	AMI_2
Fibrinolytic Therapy Received within 30 Minutes of Hospital Arrival	AMI_7a
Primary PCI Received Within 90 Minutes of Hospital Arrival	AMI_8a
Relievers for Inpatient Asthma	CAC_1
Systemic Corticosteroids for Inpatient Asthma	CAC_2
Home Management Plan of Care (HMPC) Document Given to Patient/Caregiver	CAC_3
Complication Rate Following Elective Primary  Total Hip Arthroplasty (THA) and/or Total Knee Arthroplasty (TKA)	COMP_HIP_KNEE
Median Time from ED Arrival to ED Departure for Admitted ED Patients	ED_1b
Admit Decision Time to ED Departure Time for Admitted Patients	ED_2b
Emergency Department Volume	EDV
Central Line Associated Bloodstream Infection	HAI_1
Catheter Associated Urinary Tract Infections	HAI_2
SSI - Colon Surgery	HAI_3
SSI - Abdominal Hysterectomy	HAI_4
MRSA Bacteremia	HAI_5
Clostridium Difficile (C.Diff)	HAI_6
Patient satisfaction survey results	HCAHPS
Discharge Instructions	HF_1
Evaluation of LVS Function	HF_2
ACEI or ARB for LVSD	HF_3
Influenza Immunization	IMM_2
Healthcare Personnel Influenza Vaccination	IMM_3
Acute Myocardial Infarction (AMI) 30-Day Mortality Rate	MORT_30_AMI
30-Day All-Cause Mortality Following Coronary Artery Bypass Graft (CABG) Surgery	MORT_30_CABG
Chronic Obstructive Pulmonary Disease (COPD) 30-Day Mortality Rate	MORT_30_COPD
Heart Failure (HF) 30-Day Mortality Rate	MORT_30_HF
Pneumonia 30-Day Mortality Rate	MORT_30_PN
Acute Ischemic Stroke (STK) 30-Day Mortality Rate	MORT_30_STK
Spending per Hospital Patient with Medicare (Medicare Spending per Beneficiary)	MSPB_1
Medicare Volume	MV
Median Time to Fibrinolysis	OP_1
Abdomen CT - Use of Contrast Material	OP_10
Thorax CT - Use of Contrast Material	OP_11
Does/did your facility have the ability to receive laboratory data electronically directly into your ONC certified EHR system as discrete searchable data?	OP_12
Cardiac imaging for preoperative risk assessment for non-cardiac low-risk surgery	OP_13
Simultaneous use of brain Computed Tomography (CT) and sinus Computed Tomography (CT)	OP_14
Does your facility have the ability to track clinical results between visits?	OP_17
Median Time from ED Arrival to ED Departure for Discharged ED Patients	OP_18b
Fibrinolytic Therapy Received Within 30 Minutes of ED Arrival	OP_2
Median Time from ED Arrival to Provider Contact for ED patients	OP_20
Median Time to Pain Management for Long Bone Fracture	OP_21
Patient left without being seen	OP_22
Head CT Scan Results for Acute Ischemic Stroke or Hemorrhagic Stroke Patients who Received Head CT or MRI Scan Interpretation Within 45 Minutes of ED Arrival	OP_23
Safe Surgery Checklist Use	OP_25
Hospital Outpatient Volume Data on Selected Outpatient Surgical Procedures	OP_26
Median Time to Transfer to Another Facility for Acute Coronary Intervention- Reporting Rate	OP_3b
Aspirin at Arrival	OP_4
Median Time to ECG	OP_5
Timing of Antibiotic Prophylaxis	OP_6
Prophylactic Antibiotic Selection for Surgical Patients	OP_7
MRI Lumbar Spine for Low Back Pain	OP_8
Mammography Follow-up Rates	OP_9
Risk-Standardized Payment Associated with a 30-Day AMI Episode-of-Care for Acute Myocardial Infarction	PAYM_30_AMI
Risk-Standardized Payment Associated with a 30-Day Episode of Care for Heart Failure	PAYM_30_HF
Risk-Standardized Payment Associated with a 30-Day Episode of Care for Pneumonia	PAYM_30_PN
Elective Delivery	PC_01
Initial Antibiotic Selection for CAP in Immunocompetent Patient	PN_6
Post-Operative Pulmonary Embolism (PE) or Deep Vein Thrombosis (DVT)	PSI_12
Postoperative wound dehiscence 	PSI_14
Accidental puncture or laceration  	PSI_15
Death among surgical inpatients with serious treatable complications	PSI_4
Iatrogenic pneumothorax, adult	PSI_6
Complication/patient safety for selected indicators (composite)	PSI_90
Acute Myocardial Infarction (AMI) 30-Day Readmission Rate	READM_30_AMI
30-Day All-Cause Unplanned Readmission Following Coronary Artery Bypass Graft Surgery (CABG)	READM_30_CABG
Chronic Obstructive Pulmonary Disease (COPD) 30-Day Readmission Rate	READM_30_COPD
Heart  Failure (HF) 30-Day Readmission Rate	READM_30_HF
30-Day Readmission Rate Following Elective Primary Total Hip Arthroplasty (THA) and/or Total Knee Arthroplasty (TKA)	READM_30_HIP_KNEE
30-Day Hospital-Wide All-Cause Unplanned Readmission Rate	READM_30_HOSP_WIDE
Pneumonia 30-Day Readmission Rate	READM_30_PN
Stroke (STK) 30-Day Readmission Rate	READM_30_STK
Surgery Patients on Beta-Blocker Therapy Prior to Arrival Who Received a Beta-Blocker During the Perioperative Period	SCIP_Card_2
Prophylactic Antibiotic Received Within 1 Hour Prior to Surgical Incision	SCIP_Inf_1
Surgery Patients with Perioperative Temperature Management	SCIP_Inf_10
Prophylactic Antibiotic Selection for Surgical Patients	SCIP_Inf_2
Prophylactic Antibiotics Discontinued Within 24 hours After Surgery End Time	SCIP_Inf_3
Cardiac Surgery Patients with Controlled Postoperative Blood Glucose	SCIP_Inf_4
Urinary  Catheter Removed on Postoperative Day 1 (POD 1) or Postoperative Day 2 (POD 2) with Day of Surgery being Day Zero	SCIP_Inf_9
Surgery Patients Who Received Appropriate Venous Thromboembolism Prophylaxis Within 24 Hours Prior to Surgery to 24 Hours After Surgery	SCIP_VTE_2
Participation in a Systematic Database for Cardiac Surgery	SM_PART_CARD
Participation in a Systematic Clinical Database Registry for General Surgery	SM_PART_GEN_SURG
Participation in a Systematic Clinical Database Registry for Nursing Sensitive Care	SM_PART_NURSE
Participation in a Systematic Clinical Database Registry for Stroke Care	SM_PART_STROKE
HCAHPS Summary Star Rating	Star Rating
Venous Thromboembolism (VTE) Prophylaxis	STK_1
Assessed for Rehabilitation	STK_10
Discharged on Antithrombotic Therapy	STK_2
Anticoagulation Therapy for Atrial Fibrillation/Flutter	STK_3
Thrombolytic Therapy	STK_4
Antithrombotic Therapy By End of Hospital Day 2	STK_5
Discharged on Statin Medication	STK_6
Stroke Education	STK_8
Venous Thromboembolism Prophylaxis	VTE_1
Intensive Care Unit Venous Thromboembolism Prophylaxis	VTE_2
Venous Thromboembolism Patients with Anticoagulation Overlap Therapy	VTE_3
Venous Thromboembolism Patients Receiving Unfractionated Heparin with Dosages/Platelet Count Monitoring by Protocol or Nomogram	VTE_4
Venous Thromboembolism Warfarin Therapy Discharge Instructions	VTE_5
Hospital Acquired Potentially-Preventable Venous Thromboembolism	VTE_6
Time taken: 0.062 seconds, Fetched: 100 row(s)
hive> 
    > 
    > 
    > 
    > DROP TABLE IF EXISTS hospitals;
OK
Time taken: 0.664 seconds
hive> DROP TABLE IF EXISTS hospitals;
OK
Time taken: 0.013 seconds
hive> DROP TABLE IF EXISTS effective_care;
OK
Time taken: 0.118 seconds
hive> DROP TABLE IF EXISTS readmissions;
OK
Time taken: 0.092 seconds
hive> DROP TABLE IF EXISTS measures;
OK
Time taken: 0.084 seconds
hive> DROP TABLE IF EXISTS surveys_responses;
OK
Time taken: 0.1 seconds
hive> 
Last login: Tue Oct  4 07:02:26 on console
localhost:~ NatarajanShankar$ curl https://slack.com/api/files.list?token=abcd -I
HTTP/1.1 200 OK
Content-Type: application/json; charset=utf-8
Connection: keep-alive
Access-Control-Allow-Origin: *
Content-Security-Policy: referrer no-referrer;
Date: Tue, 04 Oct 2016 16:17:44 GMT
Server: Apache
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
Vary: Accept-Encoding
X-Content-Type-Options: nosniff
X-Slack-Backend: h
X-Slack-Req-Id: 72db85b0-c132-4828-b3dd-2888d31b6c90
X-XSS-Protection: 0
X-Cache: Miss from cloudfront
Via: 1.1 f32e4aea3683be99c4324204c29f5852.cloudfront.net (CloudFront)
X-Amz-Cf-Id: VUfMKvM3KzmpQ9QZ9hSUiZ_hBg6Fjhd72hDes5szGLc6AbWxIYAnoQ==

localhost:~ NatarajanShankar$ !ssh
ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-54-227-71-81.compute-1.amazonaws.com
ssh: connect to host ec2-54-227-71-81.compute-1.amazonaws.com port 22: Operation timed out
localhost:~ NatarajanShankar$ !!
ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-54-227-71-81.compute-1.amazonaws.com
ssh: connect to host ec2-54-227-71-81.compute-1.amazonaws.com port 22: Operation timed out
localhost:~ NatarajanShankar$ ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-107-20-151-185.compute-1.amazonaws.com
The authenticity of host 'ec2-107-20-151-185.compute-1.amazonaws.com (107.20.151.185)' can't be established.
RSA key fingerprint is a8:5d:e8:8d:35:d6:cf:9c:d5:64:45:cd:8b:53:c0:a8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ec2-107-20-151-185.compute-1.amazonaws.com,107.20.151.185' (RSA) to the list of known hosts.
Last login: Tue Sep 27 22:34:43 2016 from 73.223.185.251
     ___   _        __   __   ____            __    
    / _ \ (_)___ _ / /  / /_ / __/____ ___ _ / /___ 
   / , _// // _ `// _ \/ __/_\ \ / __// _ `// // -_)
  /_/|_|/_/ \_, //_//_/\__//___/ \__/ \_,_//_/ \__/ 
           /___/                                                 
                                              
Welcome to a virtual machine image brought to you by RightScale!


[root@ip-172-31-6-204 ~]# show tables
-bash: show: command not found
[root@ip-172-31-6-204 ~]# hive
ls: cannot access /root/spark15/lib/spark-assembly-*.jar: No such file or directory

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.5.jar!/hive-log4j.properties
Exception in thread "main" java.lang.RuntimeException: java.net.ConnectException: Call From ip-172-31-6-204.ec2.internal/172.31.6.204 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:511)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Call From ip-172-31-6-204.ec2.internal/172.31.6.204 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:752)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1982)
	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1128)
	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1124)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1124)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1400)
	at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:587)
	at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:545)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:497)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 28 more
[root@ip-172-31-6-204 ~]# fdisk -l

Disk /dev/xvda1: 32.2 GB, 32212254720 bytes
255 heads, 63 sectors/track, 3916 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000


Disk /dev/xvdp: 107.4 GB, 107374182400 bytes
255 heads, 63 sectors/track, 13054 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

[root@ip-172-31-6-204 ~]# mount -t ext4 /dev/xvdp /data
[root@ip-172-31-6-204 ~]# /root/start-hadoop.sh
stopping datanode
Stopped Hadoop datanode:                                   [  OK  ]
starting datanode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-datanode-ip-172-31-6-204.out
Started Hadoop datanode (hadoop-hdfs-datanode):            [  OK  ]
no namenode to stop
Stopped Hadoop namenode:                                   [  OK  ]
starting namenode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-namenode-ip-172-31-6-204.out
Started Hadoop namenode:                                   [  OK  ]
stopping secondarynamenode
Stopped Hadoop secondarynamenode:                          [  OK  ]
starting secondarynamenode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-secondarynamenode-ip-172-31-6-204.out
Started Hadoop secondarynamenode:                          [  OK  ]
stopping resourcemanager
Stopped Hadoop resourcemanager:                            [  OK  ]
starting resourcemanager, logging to /var/log/hadoop-yarn/yarn-yarn-resourcemanager-ip-172-31-6-204.out
Started Hadoop resourcemanager:                            [  OK  ]
stopping nodemanager
Stopped Hadoop nodemanager:                                [  OK  ]
starting nodemanager, logging to /var/log/hadoop-yarn/yarn-yarn-nodemanager-ip-172-31-6-204.out
Started Hadoop nodemanager:                                [  OK  ]
stopping historyserver
Stopped Hadoop historyserver:                              [  OK  ]
starting historyserver, logging to /var/log/hadoop-mapreduce/mapred-mapred-historyserver-ip-172-31-6-204.out
16/10/04 18:54:47 INFO hs.JobHistoryServer: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting JobHistoryServer
STARTUP_MSG:   host = ip-172-31-6-204.ec2.internal/172.31.6.204
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0-cdh5.4.5
STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/aws-java-sdk-1.7.14.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/htrace-core-3.0.4.jar:/usr/lib/hadoop/lib/jasper-compiler-5.5.23.jar:/usr/lib/hadoop/lib/avro.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-el-1.0.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/jsch-0.1.42.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop/lib/zookeeper.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-collections-3.2.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/logredactor-1.0.3.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/httpclient-4.2.5.jar:/usr/lib/hadoop/lib/httpcore-4.2.5.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop/.//hadoop-common-tests.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//parquet-generator.jar:/usr/lib/hadoop/.//parquet-encoding.jar:/usr/lib/hadoop/.//hadoop-common-2.6.0-cdh5.4.5-tests.jar:/usr/lib/hadoop/.//parquet-thrift.jar:/usr/lib/hadoop/.//parquet-pig-bundle.jar:/usr/lib/hadoop/.//parquet-format-javadoc.jar:/usr/lib/hadoop/.//parquet-format.jar:/usr/lib/hadoop/.//parquet-column.jar:/usr/lib/hadoop/.//parquet-format-sources.jar:/usr/lib/hadoop/.//parquet-pig.jar:/usr/lib/hadoop/.//parquet-avro.jar:/usr/lib/hadoop/.//parquet-hadoop-bundle.jar:/usr/lib/hadoop/.//parquet-common.jar:/usr/lib/hadoop/.//hadoop-annotations-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop/.//hadoop-auth-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-aws-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//parquet-jackson.jar:/usr/lib/hadoop/.//parquet-hadoop.jar:/usr/lib/hadoop/.//parquet-cascading.jar:/usr/lib/hadoop/.//parquet-scrooge_2.10.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-nfs-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop/.//hadoop-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop/.//parquet-test-hadoop2.jar:/usr/lib/hadoop/.//parquet-protobuf.jar:/usr/lib/hadoop/.//parquet-scala_2.10.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//parquet-tools.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/htrace-core-3.0.4.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-el-1.0.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jsp-api-2.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.6.0-cdh5.4.5-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/zookeeper.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jline-2.11.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/avro.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//mockito-all-1.8.5.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//microsoft-windowsazure-storage-sdk-0.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//htrace-core-3.0.4.jar:/usr/lib/hadoop-mapreduce/.//jasper-compiler-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//avro.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.4.5-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//commons-el-1.0.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.42.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//zookeeper.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.1.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//jasper-runtime-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//avro.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.1.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-el-1.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.4.5-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/.//htrace-core-3.0.4.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//jasper-compiler-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//jasper-runtime-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.42.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//microsoft-windowsazure-storage-sdk-0.6.0.jar:/usr/lib/hadoop-mapreduce/.//mockito-all-1.8.5.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//zookeeper.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.6.0-cdh5.4.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/avro.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/modules/*.jar
STARTUP_MSG:   build = http://github.com/cloudera/hadoop -r ab14c89fe25e9fb3f9de4fb852c21365b7c5608b; compiled by 'jenkins' on 2015-08-12T21:12Z
STARTUP_MSG:   java = 1.8.0_45
************************************************************/
Started Hadoop historyserver:                              [  OK  ]
[root@ip-172-31-6-204 ~]# /data/start_postgres.sh
could not change directory to "/root"
server starting
[root@ip-172-31-6-204 ~]# pwd
/root
[root@ip-172-31-6-204 ~]# /data/start_postgres.sh
could not change directory to "/root"
pg_ctl: another server might be running; trying to start server anyway
pg_ctl: could not start server
Examine the log output.
[root@ip-172-31-6-204 ~]# ls -la
total 124
dr-xr-x--- 13 root root  4096 Oct  4 18:52 .
dr-xr-xr-x 23 root root  4096 Oct  4 18:47 ..
-rw-------  1 root root 13326 Sep 28 05:11 .bash_history
-rw-r--r--  1 root root    25 Mar 17  2015 .bash_logout
-rw-r--r--  1 root root   199 Mar 17  2015 .bash_profile
-rw-r--r--  1 root root    75 Mar 17  2015 .bashrc
drwx------  4 root root  4096 May  4  2015 .cache
drwxr-xr-x  3 root root  4096 May  4  2015 .config
-rw-------  1 root root   287 Sep 22  2015 dans_bash_history.txt
-rw-rw-r--  1 root mail   135 Sep 21 16:31 dead.letter
-rw-r--r--  1 root root   633 Oct  4 18:52 derby.log
drwxr-xr-x  3 root root  4096 May  4  2015 .emacs.d
drwxr-xr-x 11 root root  4096 May  4  2015 ipython
drwxr-xr-x  4 root root  4096 May  4  2015 .ipython
drwxr-xr-x  5 root root  4096 Oct  4 18:52 metastore_db
drwxr-xr-x  5 root root  4096 May  4  2015 pgxl-deployment-tools
drwxr-----  3 root root  4096 May  4  2015 .pki
-rw-------  1 root root   391 Sep 28  2015 .psql_history
drwxr-xr-x  3 root root  4096 May  4  2015 .python-eggs
-rwxr-xr-x  1 root root  5716 Oct 10  2015 setup_ucb_complete_plus_postgres.sh
-rw-r--r--  1 root root    77 May  4  2015 .spark_history
drwx------  2 root root  4096 Sep 21 14:18 .ssh
-rwxr-xr-x  1 root root   221 Sep 22  2015 start-hadoop.sh
-rwxr-xr-x  1 root root   252 Sep 22  2015 start-hadoop.sh~
-rwxr-xr-x  1 root root   209 Sep 22  2015 stop-hadoop.sh
-rwxr-xr-x  1 root root   222 Sep 22  2015 stop-hadoop.sh~
drwxr-xr-x 11 root root  4096 May  4  2015 streamparse
[root@ip-172-31-6-204 ~]# su - w205
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3
Found 2 items
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3/user_data
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:55 /user/w205/lab_3/weblog_data
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3/user_data
Found 1 items
-rw-r--r--   1 w205 supergroup     166205 2016-09-24 19:54 /user/w205/lab_3/user_data/userdata_lab.csv
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3/weblog_data
Found 1 items
-rw-r--r--   1 w205 supergroup    5192992 2016-09-24 19:55 /user/w205/lab_3/weblog_data/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.5.jar!/hive-log4j.properties
WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> show tables
    > ;
OK
effective_care
hospitals
measures
readmissions
surveys_responses
user_info
weblogs_flat
weblogs_parquet
weblogs_schema
Time taken: 0.568 seconds, Fetched: 9 row(s)
hive> exit
    > ;
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ bash ./setup_spark.sh
[w205@ip-172-31-6-204 ~]$ /data/start_metastore.sh
[w205@ip-172-31-6-204 ~]$ SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Starting Hive Metastore Server
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

[w205@ip-172-31-6-204 ~]$ /data/spark15/bin/spark-sql
16/10/04 19:01:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/04 19:01:10 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
16/10/04 19:01:12 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
SET hive.support.sql11.reserved.keywords=false
16/10/04 19:01:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/04 19:01:15 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
SET spark.sql.hive.version=1.2.1
SET spark.sql.hive.version=1.2.1
spark-sql> 
         > 
         > SELECT user_id, COUNT(user_id) AS log_count
         > from weblogs_schema GROUP BY user_id
         > ORDER BY log_count DESC
         > LIMIT 50;
__RequestVerificationToken_Lw__=2C2DB   10                                      
__RequestVerificationToken_Lw__=32B2B	9
__RequestVerificationToken_Lw__=A1BC3	9
__RequestVerificationToken_Lw__=B2CC1	9
__RequestVerificationToken_Lw__=3BB1C	9
__RequestVerificationToken_Lw__=111AA	9
__RequestVerificationToken_Lw__=21DCC	9
__RequestVerificationToken_Lw__=D13BD	9
__RequestVerificationToken_Lw__=113B3	9
__RequestVerificationToken_Lw__=A223D	9
__RequestVerificationToken_Lw__=2BDC3	9
__RequestVerificationToken_Lw__=3DDC1	9
__RequestVerificationToken_Lw__=233C3	9
__RequestVerificationToken_Lw__=A31AB	9
__RequestVerificationToken_Lw__=2A231	8
__RequestVerificationToken_Lw__=C2221	8
__RequestVerificationToken_Lw__=1A2CA	8
__RequestVerificationToken_Lw__=11B3B	8
__RequestVerificationToken_Lw__=B2CCB	8
__RequestVerificationToken_Lw__=B32C2	8
__RequestVerificationToken_Lw__=CB1BC	8
__RequestVerificationToken_Lw__=1A2C1	8
__RequestVerificationToken_Lw__=1133C	8
__RequestVerificationToken_Lw__=A1D22	8
__RequestVerificationToken_Lw__=DAD1D	8
__RequestVerificationToken_Lw__=2CD1D	8
__RequestVerificationToken_Lw__=AAABA	8
__RequestVerificationToken_Lw__=CBCD3	8
__RequestVerificationToken_Lw__=DA1D2	8
__RequestVerificationToken_Lw__=3AA3C	8
__RequestVerificationToken_Lw__=CA22C	8
__RequestVerificationToken_Lw__=1221A	8
__RequestVerificationToken_Lw__=12CD1	8
__RequestVerificationToken_Lw__=AA1D3	8
__RequestVerificationToken_Lw__=B13AB	8
__RequestVerificationToken_Lw__=11DBC	8
__RequestVerificationToken_Lw__=B232C	8
__RequestVerificationToken_Lw__=D1DBD	8
__RequestVerificationToken_Lw__=BD11A	8
__RequestVerificationToken_Lw__=DBBC1	8
__RequestVerificationToken_Lw__=B2B32	8
__RequestVerificationToken_Lw__=33ABD	8
__RequestVerificationToken_Lw__=31A2B	8
__RequestVerificationToken_Lw__=CA3DD	8
__RequestVerificationToken_Lw__=A1ABB	8
__RequestVerificationToken_Lw__=2A2C1	8
__RequestVerificationToken_Lw__=B1ADC	8
__RequestVerificationToken_Lw__=C23DD	7
__RequestVerificationToken_Lw__=BC1DA	7
__RequestVerificationToken_Lw__=AC3DA	7
Time taken: 11.473 seconds, Fetched 50 row(s)
spark-sql> show log_count
         > ;
NoViableAltException(26@[731:1: ddlStatement : ( createDatabaseStatement | switchDatabaseStatement | dropDatabaseStatement | createTableStatement | dropTableStatement | truncateTableStatement | alterStatement | descStatement | showStatement | metastoreCheck | createViewStatement | dropViewStatement | createFunctionStatement | createMacroStatement | createIndexStatement | dropIndexStatement | dropFunctionStatement | reloadFunctionStatement | dropMacroStatement | analyzeStatement | lockStatement | unlockStatement | lockDatabase | unlockDatabase | createRoleStatement | dropRoleStatement | ( grantPrivileges )=> grantPrivileges | ( revokePrivileges )=> revokePrivileges | showGrants | showRoleGrants | showRolePrincipals | showRoles | grantRole | revokeRole | setRole | showCurrentRole );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2586)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1650)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/10/04 19:02:43 ERROR SparkSQLDriver: Failed in [show log_count
]
org.apache.spark.sql.AnalysisException: cannot recognize input near 'show' 'log_count' '<EOF>' in ddl statement; line 1 pos 5
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
org.apache.spark.sql.AnalysisException: cannot recognize input near 'show' 'log_count' '<EOF>' in ddl statement; line 1 pos 5
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

16/10/04 19:02:43 ERROR CliDriver: org.apache.spark.sql.AnalysisException: cannot recognize input near 'show' 'log_count' '<EOF>' in ddl statement; line 1 pos 5
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

spark-sql> CREATE TABLE weblogs_parquet AS SELECT * FROM weblogs_schema
         > ;
16/10/04 19:04:19 ERROR SparkSQLDriver: Failed in [CREATE TABLE weblogs_parquet AS SELECT * FROM weblogs_schema
]
org.apache.spark.sql.AnalysisException: default.weblogs_parquet already exists.;
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.run(CreateTableAsSelect.scala:86)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:144)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:129)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
org.apache.spark.sql.AnalysisException: default.weblogs_parquet already exists.;
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.run(CreateTableAsSelect.scala:86)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:144)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:129)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

16/10/04 19:04:19 ERROR CliDriver: org.apache.spark.sql.AnalysisException: default.weblogs_parquet already exists.;
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.run(CreateTableAsSelect.scala:86)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:144)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:129)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

spark-sql> SELECT user_id, COUNT(user_id) AS log_count
         > from weblogs_parquet GROUP BY user_id
         > ORDER BY log_count DESC
         > LIMIT 50;
__RequestVerificationToken_Lw__=2C2DB   10                                      
__RequestVerificationToken_Lw__=32B2B	9
__RequestVerificationToken_Lw__=A1BC3	9
__RequestVerificationToken_Lw__=B2CC1	9
__RequestVerificationToken_Lw__=111AA	9
__RequestVerificationToken_Lw__=3BB1C	9
__RequestVerificationToken_Lw__=3DDC1	9
__RequestVerificationToken_Lw__=113B3	9
__RequestVerificationToken_Lw__=21DCC	9
__RequestVerificationToken_Lw__=A223D	9
__RequestVerificationToken_Lw__=D13BD	9
__RequestVerificationToken_Lw__=2BDC3	9
__RequestVerificationToken_Lw__=233C3	9
__RequestVerificationToken_Lw__=A31AB	9
__RequestVerificationToken_Lw__=2A231	8
__RequestVerificationToken_Lw__=3AA3C	8
__RequestVerificationToken_Lw__=1A2CA	8
__RequestVerificationToken_Lw__=11B3B	8
__RequestVerificationToken_Lw__=31A2B	8
__RequestVerificationToken_Lw__=33ABD	8
__RequestVerificationToken_Lw__=1221A	8
__RequestVerificationToken_Lw__=1A2C1	8
__RequestVerificationToken_Lw__=1133C	8
__RequestVerificationToken_Lw__=A1D22	8
__RequestVerificationToken_Lw__=DAD1D	8
__RequestVerificationToken_Lw__=2CD1D	8
__RequestVerificationToken_Lw__=DBBC1	8
__RequestVerificationToken_Lw__=AAABA	8
__RequestVerificationToken_Lw__=B2CCB	8
__RequestVerificationToken_Lw__=DA1D2	8
__RequestVerificationToken_Lw__=C2221	8
__RequestVerificationToken_Lw__=CA3DD	8
__RequestVerificationToken_Lw__=12CD1	8
__RequestVerificationToken_Lw__=A1ABB	8
__RequestVerificationToken_Lw__=B13AB	8
__RequestVerificationToken_Lw__=11DBC	8
__RequestVerificationToken_Lw__=B232C	8
__RequestVerificationToken_Lw__=D1DBD	8
__RequestVerificationToken_Lw__=BD11A	8
__RequestVerificationToken_Lw__=B2B32	8
__RequestVerificationToken_Lw__=CBCD3	8
__RequestVerificationToken_Lw__=CA22C	8
__RequestVerificationToken_Lw__=B32C2	8
__RequestVerificationToken_Lw__=CB1BC	8
__RequestVerificationToken_Lw__=AA1D3	8
__RequestVerificationToken_Lw__=2A2C1	8
__RequestVerificationToken_Lw__=B1ADC	8
__RequestVerificationToken_Lw__=C23DD	7
__RequestVerificationToken_Lw__=D133C	7
__RequestVerificationToken_Lw__=BC1DA	7
Time taken: 3.646 seconds, Fetched 50 row(s)
spark-sql> get tables
         > ;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1071)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/10/04 19:24:07 ERROR SparkSQLDriver: Failed in [get tables
]
org.apache.spark.sql.AnalysisException: cannot recognize input near 'get' 'tables' '<EOF>'; line 1 pos 0
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
org.apache.spark.sql.AnalysisException: cannot recognize input near 'get' 'tables' '<EOF>'; line 1 pos 0
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

16/10/04 19:24:07 ERROR CliDriver: org.apache.spark.sql.AnalysisException: cannot recognize input near 'get' 'tables' '<EOF>'; line 1 pos 0
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:305)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:224)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

spark-sql> exit
         > ;
[w205@ip-172-31-6-204 ~]$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.5.jar!/hive-log4j.properties


WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> 
    > 
    > getr tables
    > ;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1025)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:394)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1111)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1159)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1048)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1038)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:0 cannot recognize input near 'getr' 'tables' '<EOF>'
hive> get tables;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1025)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:394)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1111)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1159)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1048)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1038)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:0 cannot recognize input near 'get' 'tables' '<EOF>'
hive> GET tables;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1025)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:394)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1111)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1159)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1048)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1038)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:0 cannot recognize input near 'GET' 'tables' '<EOF>'
hive> GET TABLES;
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1025)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:199)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:394)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1111)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1159)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1048)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1038)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:0 cannot recognize input near 'GET' 'TABLES' '<EOF>'
hive> show tables;
OK
effective_care
hospitals
measures
readmissions
surveys_responses
user_info
weblogs_flat
weblogs_parquet
weblogs_schema
Time taken: 0.24 seconds, Fetched: 9 row(s)
hive> CREATE EXTERNAL TABLE IF NOT EXISTS hospitals
    > (hospitals string)
    > ROW FORMAT DELIMITED
    > STORED AS TEXTFILE
    > exit;
FAILED: ParseException line 5:0 extraneous input 'exit' expecting EOF near '<EOF>'
hive> exit;
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3/weblog_data
Found 1 items
-rw-r--r--   1 w205 supergroup    5192992 2016-09-24 19:55 /user/w205/lab_3/weblog_data/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3            
Found 2 items
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3/user_data
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:55 /user/w205/lab_3/weblog_data
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/     
Found 6 items
-rw-r--r--   1 w205 supergroup        693 2016-09-21 14:46 /user/w205/derby.log
-rw-r--r--   1 w205 supergroup       4103 2016-09-23 15:20 /user/w205/hive_base_ddl.sql
drwxr-xr-x   - w205 supergroup          0 2016-09-23 15:21 /user/w205/hospital_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-22 17:52 /user/w205/hospitals_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3
-rw-r--r--   1 w205 supergroup       1869 2016-09-22 17:58 /user/w205/load_data_lake.sh
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospital_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205                 
Found 6 items
-rw-r--r--   1 w205 supergroup        693 2016-09-21 14:46 /user/w205/derby.log
-rw-r--r--   1 w205 supergroup       4103 2016-09-23 15:20 /user/w205/hive_base_ddl.sql
drwxr-xr-x   - w205 supergroup          0 2016-09-23 15:21 /user/w205/hospital_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-22 17:52 /user/w205/hospitals_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3
-rw-r--r--   1 w205 supergroup       1869 2016-09-22 17:58 /user/w205/load_data_lake.sh
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospitals_compare
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ exit
logout
[root@ip-172-31-6-204 ~]# pwd
/root
[root@ip-172-31-6-204 ~]# ls -l
total 52
-rw-------  1 root root  287 Sep 22  2015 dans_bash_history.txt
-rw-rw-r--  1 root mail  135 Sep 21 16:31 dead.letter
-rw-r--r--  1 root root  633 Oct  4 18:52 derby.log
drwxr-xr-x 11 root root 4096 May  4  2015 ipython
drwxr-xr-x  5 root root 4096 Oct  4 18:52 metastore_db
drwxr-xr-x  5 root root 4096 May  4  2015 pgxl-deployment-tools
-rwxr-xr-x  1 root root 5716 Oct 10  2015 setup_ucb_complete_plus_postgres.sh
-rwxr-xr-x  1 root root  221 Sep 22  2015 start-hadoop.sh
-rwxr-xr-x  1 root root  252 Sep 22  2015 start-hadoop.sh~
-rwxr-xr-x  1 root root  209 Sep 22  2015 stop-hadoop.sh
-rwxr-xr-x  1 root root  222 Sep 22  2015 stop-hadoop.sh~
drwxr-xr-x 11 root root 4096 May  4  2015 streamparse
[root@ip-172-31-6-204 ~]# cd /data
[root@ip-172-31-6-204 data]# ls -la
total 68
drwxrwxrwx  8 root     root    4096 Sep 26 17:42 .
dr-xr-xr-x 23 root     root    4096 Oct  4 18:47 ..
drwxr-xr-x  3 hadoop   hadoop  4096 Sep 21 14:44 hadoop
drwxr-xr-x  3 hdfs     hdfs    4096 Sep 21 14:39 hadoop-hdfs
drwxr-xr-x  4 yarn     yarn    4096 Sep 21 14:39 hadoop-yarn
drwx------  2 root     root   16384 Sep 21 14:38 lost+found
drwxr-xr-x  4 postgres root    4096 Sep 21 14:43 pgsql
-rw-r--r--  1 root     root     535 Sep 21 14:44 setup_hive_for_postgres.sql
-rwxr-xr-x  1 root     root     732 Sep 21 14:44 setup_zeppelin.sh
drwxr-xr-x 11 w205     w205    4096 Sep 26 17:42 spark15
-rwxrwxr-x  1 w205     w205      27 Oct  4 18:59 start_metastore.sh
-rwxr-xr-x  1 root     root      93 Sep 21 14:44 start_postgres.sh
-rwxrwxr-x  1 w205     w205      94 Oct  4 18:59 stop_metastore.sh
-rwxr-xr-x  1 root     root      92 Sep 21 14:44 stop_postgres.sh
[root@ip-172-31-6-204 data]# cd ~
[root@ip-172-31-6-204 ~]# pwd
/root
[root@ip-172-31-6-204 ~]# whoami
root
[root@ip-172-31-6-204 ~]# su - w205
[w205@ip-172-31-6-204 ~]$ ls -l
total 279840
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospital_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/weblog*         
ls: `/user/w205/weblog*': No such file or directory
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/weblog_data
ls: `/user/w205/weblog_data': No such file or directory
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/lab_3      
Found 2 items
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3/user_data
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:55 /user/w205/lab_3/weblog_data
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospital_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospitals_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205                  
Found 6 items
-rw-r--r--   1 w205 supergroup        693 2016-09-21 14:46 /user/w205/derby.log
-rw-r--r--   1 w205 supergroup       4103 2016-09-23 15:20 /user/w205/hive_base_ddl.sql
drwxr-xr-x   - w205 supergroup          0 2016-09-23 15:21 /user/w205/hospital_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-22 17:52 /user/w205/hospitals_compare
drwxr-xr-x   - w205 supergroup          0 2016-09-24 19:54 /user/w205/lab_3
-rw-r--r--   1 w205 supergroup       1869 2016-09-22 17:58 /user/w205/load_data_lake.sh
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ exit
logout
[root@ip-172-31-6-204 ~]# pwd
/root
[root@ip-172-31-6-204 ~]# ls -l
total 52
-rw-------  1 root root  287 Sep 22  2015 dans_bash_history.txt
-rw-rw-r--  1 root mail  135 Sep 21 16:31 dead.letter
-rw-r--r--  1 root root  633 Oct  4 18:52 derby.log
drwxr-xr-x 11 root root 4096 May  4  2015 ipython
drwxr-xr-x  5 root root 4096 Oct  4 18:52 metastore_db
drwxr-xr-x  5 root root 4096 May  4  2015 pgxl-deployment-tools
-rwxr-xr-x  1 root root 5716 Oct 10  2015 setup_ucb_complete_plus_postgres.sh
-rwxr-xr-x  1 root root  221 Sep 22  2015 start-hadoop.sh
-rwxr-xr-x  1 root root  252 Sep 22  2015 start-hadoop.sh~
-rwxr-xr-x  1 root root  209 Sep 22  2015 stop-hadoop.sh
-rwxr-xr-x  1 root root  222 Sep 22  2015 stop-hadoop.sh~
drwxr-xr-x 11 root root 4096 May  4  2015 streamparse
[root@ip-172-31-6-204 ~]# su - w205
[w205@ip-172-31-6-204 ~]$ ls -l
total 279840
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ whoami
w205
[w205@ip-172-31-6-204 ~]$ pwd  
/home/w205
[w205@ip-172-31-6-204 ~]$ ls -l
total 279840
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ ls -l
total 279840
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ ls -l
total 363356
-rw-r--r--  1 root root     13146 Oct  4 22:49 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  4 22:50 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  4 22:50 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  4 22:50 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ hdfs dfs -mkdir /user/w205/exercise_1/hospitals_compare
mkdir: `/user/w205/exercise_1/hospitals_compare': No such file or directory
[w205@ip-172-31-6-204 ~]$ hdfs dfs -mkdir /user/w205/exercise_1                  
[w205@ip-172-31-6-204 ~]$ hdfs dfs -mkdir /user/w205/exercise_1/hospitals_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -mkdir /user/w205/exercise_1/hospital_compare
[w205@ip-172-31-6-204 ~]$ hdfs dfs -put *.csv /user/w205/exercise_1/hospital_compare/.
[w205@ip-172-31-6-204 ~]$ hdfs dfs -put *.csv /user/w205/exercise_1/hospitals_compare/.
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/exercise_1/hospitals_compare  
Found 7 items
-rw-r--r--   1 w205 supergroup      13146 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/Measures.csv
-rw-r--r--   1 w205 supergroup   63280769 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/effective_care.csv
-rw-r--r--   1 w205 supergroup     826758 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/hospitals.csv
-rw-r--r--   1 w205 supergroup   19936145 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/readmissions.csv
-rw-r--r--   1 w205 supergroup    1348499 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/surveys_responses.csv
-rw-r--r--   1 w205 supergroup     166205 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/userdata_lab.csv
-rw-r--r--   1 w205 supergroup    5192992 2016-10-04 22:53 /user/w205/exercise_1/hospitals_compare/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/exercise_1/hospital_compare
Found 7 items
-rw-r--r--   1 w205 supergroup      13146 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/Measures.csv
-rw-r--r--   1 w205 supergroup   63280769 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/effective_care.csv
-rw-r--r--   1 w205 supergroup     826758 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/hospitals.csv
-rw-r--r--   1 w205 supergroup   19936145 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/readmissions.csv
-rw-r--r--   1 w205 supergroup    1348499 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/surveys_responses.csv
-rw-r--r--   1 w205 supergroup     166205 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/userdata_lab.csv
-rw-r--r--   1 w205 supergroup    5192992 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ vi /user/w205/exercise_1/hospital_compare/hospitals.csv
[w205@ip-172-31-6-204 ~]$ ls -l
total 363356
-rw-r--r--  1 root root     13146 Oct  4 22:49 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  4 22:50 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  4 22:50 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  4 22:50 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ vi hive_base_ddl.sql
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ 
[w205@ip-172-31-6-204 ~]$ !vi
vi hive_base_ddl.sql
[w205@ip-172-31-6-204 ~]$ ls -l
total 363356
-rw-r--r--  1 root root     13146 Oct  4 22:49 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  4 22:50 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  4 22:50 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  4 22:50 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ whoami
w205
[w205@ip-172-31-6-204 ~]$ chown w205 *.csv
chown: changing ownership of `Measures.csv': Operation not permitted
chown: changing ownership of `effective_care.csv': Operation not permitted
chown: changing ownership of `hospitals.csv': Operation not permitted
chown: changing ownership of `readmissions.csv': Operation not permitted
chown: changing ownership of `surveys_responses.csv': Operation not permitted
[w205@ip-172-31-6-204 ~]$ su
Password: 
su: incorrect password
[w205@ip-172-31-6-204 ~]$ su - root
Password: 
su: incorrect password
[w205@ip-172-31-6-204 ~]$ ls -l
total 363356
-rw-r--r--  1 root root     13146 Oct  4 22:49 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  4 22:50 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  4 22:50 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  4 22:50 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ ls -la
total 363400
drwx------  4 w205 w205      4096 Oct  4 23:10 .
drwxr-xr-x  7 root root      4096 Sep 22  2015 ..
-rw-------  1 w205 w205      5474 Oct  4 21:20 .bash_history
-rw-r--r--  1 w205 w205        18 Oct 16  2014 .bash_logout
-rw-r--r--  1 w205 w205       176 Oct 16  2014 .bash_profile
-rw-r--r--  1 w205 w205       124 Oct 16  2014 .bashrc
-rw-r--r--  1 w205 w205       500 May  7  2013 .emacs
-rw-------  1 w205 w205        16 Sep 28  2015 .psql_history
-rw-------  1 w205 w205      4271 Oct  4 23:10 .viminfo
-rw-r--r--  1 root root     13146 Oct  5 00:31 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  5 00:31 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  4 22:50 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  4 22:50 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ ls -la
total 363400
drwx------  4 w205 w205      4096 Oct  4 23:10 .
drwxr-xr-x  7 root root      4096 Sep 22  2015 ..
-rw-------  1 w205 w205      5474 Oct  4 21:20 .bash_history
-rw-r--r--  1 w205 w205        18 Oct 16  2014 .bash_logout
-rw-r--r--  1 w205 w205       176 Oct 16  2014 .bash_profile
-rw-r--r--  1 w205 w205       124 Oct 16  2014 .bashrc
-rw-r--r--  1 w205 w205       500 May  7  2013 .emacs
-rw-------  1 w205 w205        16 Sep 28  2015 .psql_history
-rw-------  1 w205 w205      4271 Oct  4 23:10 .viminfo
-rw-r--r--  1 root root     13146 Oct  5 00:31 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  5 00:32 effective_care.csv
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  5 00:32 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  5 00:32 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  4 22:50 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ ls -l *.csv
-rw-r--r-- 1 root root    13146 Oct  5 00:31 Measures.csv
-rw-r--r-- 1 root root 63280769 Oct  5 00:32 effective_care.csv
-rw-r--r-- 1 root root   826758 Oct  5 00:32 hospitals.csv
-rw-r--r-- 1 root root 19936145 Oct  5 00:32 readmissions.csv
-rw-r--r-- 1 root root  1348499 Oct  5 00:32 surveys_responses.csv
-rw-rw-r-- 1 w205 w205   166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r-- 1 w205 w205  5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ rm hospitals.csv 
rm: remove write-protected regular file `hospitals.csv'? yes
[w205@ip-172-31-6-204 ~]$ rm effective_care.csv 
rm: remove write-protected regular file `effective_care.csv'? yes
[w205@ip-172-31-6-204 ~]$ rm Measures.csv 
rm: remove write-protected regular file `Measures.csv'? yes
[w205@ip-172-31-6-204 ~]$ rm readmissions.csv 
rm: remove write-protected regular file `readmissions.csv'? yes
[w205@ip-172-31-6-204 ~]$ ls -l
total 281164
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root      4103 Sep 23 14:28 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  5 00:32 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ rm surveys_responses.csv 
rm: remove write-protected regular file `surveys_responses.csv'? yes
[w205@ip-172-31-6-204 ~]$ vi hive_base_ddl.sql 

[No write since last change]
/bin/bash: wq: command not found

shell returned 127

Press ENTER or type command to continue
[No write since last change]
 00:35:13 up  5:47,  2 users,  load average: 0.00, 0.00, 0.00
USER     TTY      FROM              LOGIN@   IDLE   JCPU   PCPU WHAT
root     pts/0    73.223.185.251   18:52    0.00s 22.94s  0.02s vim hive_base_ddl.sql
root     pts/1    73.223.185.251   23:10    1:24m  0.02s  0.02s vim hive_base_ddl.sql

Press ENTER or type command to continue
[w205@ip-172-31-6-204 ~]$ !vi
vi hive_base_ddl.sql 
[w205@ip-172-31-6-204 ~]$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.5.jar!/hive-log4j.properties
WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> DROP TABLE IF EXISTS hospitals;
OK
Time taken: 0.903 seconds
hive> CREATE EXTERNAL TABLE   hospitals(
    >                 Provider_ID             string,
    >                 Name                    string,
    >                 Address                 string,
    >                 City                    string,
    >                 State                   string,
    >                 ZIP_CODE                string,
    >                 County                  string,
    >                 Phone_Number            string,
    >                 Hospital_Type           string,
    >                 Ownership               string,
    >                 Emergency_Services      string
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
OK
Time taken: 0.192 seconds
hive> DROP TABLE IF EXISTS effective_care;
OK
Time taken: 0.106 seconds
hive> CREATE EXTERNAL TABLE effective_care(
    >                 Provider_ID             string,
    >                 Name                    string,
    >                 Address                 string,
    >                 City                    string,
    >                 State                   string,
    >                 ZIP_CODE                string,
    >                 County                  string,
    >                 Phone_Number            string,
    >                 Measure_ID              string,
    >                 Measure_Name            string,
    >                 Score                   int,
    >                 Sample                  int,
    >                 Footnote                string,
    >                 Start_Date              date,
    >                 End_Date                date
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
OK
Time taken: 0.073 seconds
hive> DROP TABLE IF EXISTS readmissions;
OK
Time taken: 0.103 seconds
hive> CREATE EXTERNAL TABLE readmissions(
    >                 Provider_ID             string,
    >                 Name                    string,
    >                 Address                 string,
    >                 City                    string,
    >                 State                   string,
    >                 ZIP_CODE                string,
    >                 County                  string,
    >                 Phone_Number            string,
    >                 Measure_ID              string,
    >                 Measure_Name            string,
    >                 Compared_to_National    string,
    >                 Denominator             string,
    >                 Score                   int,
    >                 Lower_Estimate          int,
    >                 Higher_Estimate         int,
    >                 Footnote                string,
    >                 Start_Date              date,
    >                 End_Date                date
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\' )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
OK
Time taken: 0.06 seconds
hive> DROP TABLE IF EXISTS measures;
OK
Time taken: 0.085 seconds
hive> CREATE EXTERNAL TABLE measures(
    >                 Measure_Name            string,
    >                 Measure_ID              string,
    >                 Measure_StartQuarter    string,
    >                 Measure_Start_Date      datetime,
    >                 Measure_EndQuarter      string,
    >                 Measure_End_Date        datetime
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > exit;
FAILED: ParseException line 16:0 extraneous input 'exit' expecting EOF near '<EOF>'
hive> DROP TABLE IF EXISTS measures;
OK
Time taken: 0.02 seconds
hive> CREATE EXTERNAL TABLE measures(
    >                 Measure_Name            string,
    >                 Measure_ID              string,
    >                 Measure_StartQuarter    string,
    >                 Measure_Start_Date      datetime,
    >                 Measure_End_Quarter     string,
    >                 Measure_End_Date        datetime
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
FAILED: SemanticException [Error 10099]: DATETIME type isn't supported yet. Please use DATE or TIMESTAMP instead
hive> DROP TABLE IF EXISTS measures;
OK
Time taken: 0.011 seconds
hive> CREATE EXTERNAL TABLE measures(
    >                 Measure_Name            string,
    >                 Measure_ID              string,
    >                 Measure_StartQuarter    string,
    >                 Measure_Start_Date      timestamp,
    >                 Measure_End_Quarter     string,
    >                 Measure_End_Date        datetime
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
FAILED: SemanticException [Error 10099]: DATETIME type isn't supported yet. Please use DATE or TIMESTAMP instead
hive> DROP TABLE IF EXISTS measures;
OK
Time taken: 0.009 seconds
hive> CREATE EXTERNAL TABLE measures(
    >                 Measure_Name            string,
    >                 Measure_ID              string,
    >                 Measure_StartQuarter    string,
    >                 Measure_Start_Date      timestamp,
    >                 Measure_End_Quarter     string,
    >                 Measure_End_Date        timestamp
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    > WITH SERDEPROPERTIES(
    >                 "separatorChar" = ',',
    >                 "quoteChar" = '"',
    >                 "escapeChar" = '\\'
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
OK
Time taken: 0.055 seconds
hive> DROP TABLE IF EXISTS surveys_responses;
OK
Time taken: 0.119 seconds
hive> CREATE EXTERNAL TABLE surveys_responses(
    >                 Provider_ID             string,
    >                 Name                    string,
    >                 Address                 string,
    >                 City                    string,
    >                 State                   string,
    >                 ZIP_CODE                string,
    >                 County                  string,
    >                 Communication_with_Nurses_Achievement_Points    string,
    >                 Communication_with_Nurses_Improvement_Points    string,
    >                 Communication_with_Nurses_Dimension_Score       string,
    >                 Communication_with_Doctors_Achievement_Points   string,
    >                 Communication_with_Doctors_Improvement_Points   string,
    >                 Communication_with_Doctors_Dimension_Score      string,
    >                 Responsiveness_of_Hospital_Staff_Achievement_Points     string,
    >                 Responsiveness_of_Hospital_Staff_Improvement_Points     string,
    >                 Responsiveness_of_Hospital_Staff_Dimension_Score        string,
    >                 Pain_Management_Achievement_Points              string,
    >                 Pain_Management_Improvement_Points              string,
    >                 Pain_Management_Dimension_Score                 string,
    >                 Communication_about_Medicines_Achievement_Points        string,
    >                 Communication_about_Medicines_Improvement_Points        string,
    >                 Communication_about_Medicines_Dimension_Score           string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Achievement_Points    string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Improvement_Points    string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Dimension_Score       string,
    >                 Discharge_Information_Achievement_Points        string,
    >                 Discharge_Information_Improvement_Points        string,
    >                 Discharge_Information_Dimension_Score           string,
    >                 Overall_Rating_of_Hospital_Achievement_Points   string,
    >                 Overall_Rating_of_Hospital_Improvement_Points   string,
    >                 Overall_Rating_of_Hospital_Dimension_Score      string,
    >                 HCAHPS_Base_Score                               INT,
    >                 HCAHPS_Consistency_Score                        INT
    > )
    > STORED AS TEXTFILE
    > 
    > LOCSATION '/user/w205/hospital_compare/';
FAILED: ParseException line 38:0 missing EOF at 'LOCSATION' near 'TEXTFILE'
hive> DROP TABLE IF EXISTS surveys_responses;
OK
Time taken: 0.009 seconds
hive> CREATE EXTERNAL TABLE surveys_responses(
    >                 Provider_ID             string,
    >                 Name                    string,
    >                 Address                 string,
    >                 City                    string,
    >                 State                   string,
    >                 ZIP_CODE                string,
    >                 County                  string,
    >                 Communication_with_Nurses_Achievement_Points    string,
    >                 Communication_with_Nurses_Improvement_Points    string,
    >                 Communication_with_Nurses_Dimension_Score       string,
    >                 Communication_with_Doctors_Achievement_Points   string,
    >                 Communication_with_Doctors_Improvement_Points   string,
    >                 Communication_with_Doctors_Dimension_Score      string,
    >                 Responsiveness_of_Hospital_Staff_Achievement_Points     string,
    >                 Responsiveness_of_Hospital_Staff_Improvement_Points     string,
    >                 Responsiveness_of_Hospital_Staff_Dimension_Score        string,
    >                 Pain_Management_Achievement_Points              string,
    >                 Pain_Management_Improvement_Points              string,
    >                 Pain_Management_Dimension_Score                 string,
    >                 Communication_about_Medicines_Achievement_Points        string,
    >                 Communication_about_Medicines_Improvement_Points        string,
    >                 Communication_about_Medicines_Dimension_Score           string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Achievement_Points    string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Improvement_Points    string,
    >                 Cleanliness_and_Quietness_of_Hospital_Environment_Dimension_Score       string,
    >                 Discharge_Information_Achievement_Points        string,
    >                 Discharge_Information_Improvement_Points        string,
    >                 Discharge_Information_Dimension_Score           string,
    >                 Overall_Rating_of_Hospital_Achievement_Points   string,
    >                 Overall_Rating_of_Hospital_Improvement_Points   string,
    >                 Overall_Rating_of_Hospital_Dimension_Score      string,
    >                 HCAHPS_Base_Score                               INT,
    >                 HCAHPS_Consistency_Score                        INT
    > )
    > STORED AS TEXTFILE
    > LOCATION '/user/w205/hospital_compare/';
OK
Time taken: 0.106 seconds
hive> SELECT Provider_ID, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals 
    > ORDER BY hospital_count
    > LIMIT 100;
FAILED: SemanticException [Error 10025]: Line 1:7 Expression not in GROUP BY key 'Provider_ID'
hive> SELECT Provider_ID, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals 
    > GROUP BY Provider_ID
    > ORDER BY hospital_count
    > LIMIT 100;
Query ID = w205_20161005005454_1b1da24f-d111-47a6-beff-b6f98eeeb6e0
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0001, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0001/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-10-05 00:54:31,655 Stage-1 map = 0%,  reduce = 0%
2016-10-05 00:54:38,280 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.19 sec
2016-10-05 00:54:44,663 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.85 sec
MapReduce Total cumulative CPU time: 2 seconds 850 msec
Ended Job = job_1475607266406_0001
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0002, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0002/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0002
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2016-10-05 00:54:55,210 Stage-2 map = 0%,  reduce = 0%
2016-10-05 00:55:01,736 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.53 sec
2016-10-05 00:55:08,080 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.97 sec
MapReduce Total cumulative CPU time: 2 seconds 970 msec
Ended Job = job_1475607266406_0002
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.85 sec   HDFS Read: 7464 HDFS Write: 96 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.97 sec   HDFS Read: 4548 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 820 msec
OK
Time taken: 50.151 seconds
hive> SELECT Provider_ID, COUNT(Provider_ID) AS hospital_count
    > GROUP BY Provider_ID
    > LIMIT 100;
FAILED: SemanticException [Error 10004]: Line 2:9 Invalid table alias or column reference 'Provider_ID': (possible column names are: )
hive> SELECT Provider_ID FROM hospitals;
OK
Time taken: 0.057 seconds
hive> SELECT Provider_ID FROM hospitals
    > LIMIT 100;
OK
Time taken: 0.051 seconds
hive> SELECT * FROM hospitals
    > LIMIT 100;
OK
Time taken: 0.075 seconds
hive> exit
    > ;
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ hdfs dfs -l /user/w205/hospital_compare
-l: Unknown command
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospital_compare
[w205@ip-172-31-6-204 ~]$ history               
    1  wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.0-bin-hadoop2.6.tgz
    2  tar xvzf spark-1.5.0-bin-hadoop2.6.tgz
    3  mv spark-1.5.0-bin-hadoop2.6 spark15
    4  $SPARK_HOME/bin/pyspark --master yarn
    5  exit
    6  export HIVE_CONF_DIR=/data/hadoop/hive/conf
    7  hive
    8  exit
    9  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   10  hive
   11  psql -U hiveuser -d metastore
   12  ifconfig
   13  exit
   14  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   15  hive
   16  exit
   17  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   18  hive
   19  exit
   20  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   21  hive –e “s'ow tables;?
   22  exit
   23  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   24  hive --help
   25  hive --service
   26  exit
   27  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   28  hive
   29  exit
   30  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   31  hive
   32  exit
   33  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   34  hive
   35  exit
   36  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   37  hive
   38  exit
   39  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   40  hive
   41  exit;
   42  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   43  hive
   44  exit
   45  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   46  hive
   47  psql -h localhost -U hiveuser -d metastore
   48  exit
   49  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   50  hive
   51  exit;
   52  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   53  hive
   54  hive --service metastore
   55  service hive-metastore start
   56  ls /etc/init.d/*hive*
   57  exit
   58  hive
   59  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   60  hive
   61  exit
   62  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   63  hive -e 'show tables;'
   64  exit
   65  export HIVE_CONF_DIR=/data/hadoop/hive/conf
   66  /home/w205/spark15/bin/spark-sql 
   67  exit;
   68  psql -d w205
   69  exit;
   70  pwd
   71  ls -alF
   72  ls -la
   73  hdfs dfs -ls /user
   74  hdfs dfs -put derby.log /user/w205
   75  hdfs dfs -ls /user/w205
   76  hdfs dfs ls /user
   77  hdfs dfs -ls /user
   78  pwd
   79  hdfs dfs -mkdir /user/w205/hospital_compare
   80  hdfs dfs -ls /user/w205
   81  whoami
   82  pwd
   83  ls -la
   84  pwd
   85  cd /user/w205
   86  pwd
   87  whoami
   88  pwd
   89  whamil
   90  whoami
   91  exit
   92  pwd
   93  exit
   94  hdfs dfs -put /root/.csv /user/w205/.
   95  hdfs dfs -put /root/*.csv /user/w205/.
   96  cd /root
   97  exit
   98  pwd
   99  cd /user/w205
  100  hdfs dfs -l /user
  101  hdfs dfs -ls /user
  102  cd /home
  103  ls -l
  104  cd w205
  105  whoami
  106  ls -l
  107  ls -la
  108  pwd
  109  ls -la
  110  tie
  111  time
  112  clock
  113  date
  114  sudo tzselect
  115  pwd
  116  ls -l
  117  hdfs dfs -ls /user/w205
  118  hdfs dfs -ls /user/w205/hosp*
  119  hdfs dfs -put *.csv /user/w205/hospital_compare/.
  120  hdfs dfs -ls /user/w205/hosp*
  121  ped
  122  pwd
  123  ls -l
  124  ls-la
  125  ls -la
  126  hdfd dfs -ls /home/w205/hosp*
  127  hdfs dfs -ls /home/w205/hosp*
  128  hdfs dfs -ls /user/w205/hosp*
  129  exit
  130  pwd
  131  ls -l
  132  cd /user/w205
  133  exit
  134  hdfs dfs -rm /user/w205/hospital_compare/*
  135  hdfs dfs -ls /user/w205/hospital_compare
  136  hdfs dfs -rmdir /user/w205/hospital_compare
  137  hdfs dfs -mkdir /user/w205/hospitals_compare
  138  hdfs dfs -ls /user/w205/
  139  exit
  140  hdfs dfs -put load_data_lake.sh /user/w205
  141  hdfs dfs -ls /user/w205
  142  pwd
  143  ls -l
  144  hdfs dfs put hive_base_ddl.sql /user/w205
  145  hdfs dfs -put hive_base_ddl.sql /user/w205
  146  cd /user/w205
  147  pwd
  148  hive -f ./hive_base_ddl.sql
  149  ls -l
  150  [w205@ip-172-31-6-204 ~]$ hive -f ./hive_base_ddl.sql
  151  SLF4J: Class path contains multiple SLF4J bindings.
  152  SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
  153  SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
  154  SLF4J: Class path contains multiple SLF4J bindings.
  155  SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
  156  SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
  157  OK
  158  Time taken: 0.463 seconds
  159  OK
  160  Time taken: 0.332 seconds
  161  OK
  162  Time taken: 0.01 seconds
  163  OK
  164  Time taken: 0.053 seconds
  165  OK
  166  Time taken: 0.009 seconds
  167  OK
  168  Time taken: 0.069 seconds
  169  OK
  170  Time taken: 0.012 seconds
  171  OK
  172  Time taken: 0.042 seconds
  173  OK
  174  Time taken: 0.009 seconds
  175  OK
  176  Time taken: 0.093 seconds
  177  hdfs dfs -mkdir /user/w205/lab_3
  178  wget https://s3.amazonaws.com/ucbdatasciencew205/lab_datasets/userdata_lab.csv
  179  wget https://s3.amazonaws.com/ucbdatasciencew205/lab_datasets/weblog_lab.csv
  180  ls -la
  181  hdfs dfs -mkdir /user/w205/lab_3/user_data
  182  hdfs dfs -mkdir /user/w205/lab_3/weblog_data
  183  hdfs dfs -put userdata_lab.csv /user/w205/lab_3/user_data
  184  hdfs dfs -put weblog_lab.csv /user/w205/lab_3/weblog_data
  185  ls -la
  186  cat userdata_lab.csv 
  187  cat weblog_lab.csv 
  188  hive
  189  whoami
  190  hdfs dfs -ls -la /user/w205/lab_3
  191  hdfs dfs -ls /user/w205/lab_3
  192  hdfs dfs -ls /user/w205/lab_3/user_data
  193  hdfs dfs -ls /user/w205/lab_3/weblog_data
  194  quit
  195  exit
  196  hive
  197  exit
  198  hive
  199  pwd
  200  SPARK SQL CLI
  201  wget https://s3.awazonaws.com/ucbdatasciencew205/setup_spark.sh
  202  wget https://s3.amazonaws.com/ucbdatasciencew205/setup_spark.sh
  203  ls -la
  204  vi setup_spark.sh
  205  bash ./setup_spark.sh
  206  vi setup_spark.sh
  207  ls -l /data
  208  cat start_metastore.sh
  209  cat /data/start_metastore.sh
  210  /data/start_metastore.sh
  211  /data/spark15/bin/spark-sql
  212  pwd
  213  ls -la
  214  cd spark15
  215  ls -l
  216  cd conf
  217  ls -l
  218  vi spark-defaults.conf.template
  219  fgrep rootCategory *
  220  vi log4j.properties.template
  221  pwd
  222  cd
  223  pwd
  224  ls -l
  225  history
  226  sc.setLogLevel("ERROR")
  227  sc.setLogLevel("INFO")
  228  /data/spark15/bin/spark-sql
  229  spark-shell
  230  export PATH = $PATH:/usr/local/spark/bin
  231  ls -l
  232  ls -l spark15
  233  cat README.md
  234  cd spark15
  235  cat README.md
  236  ls -la
  237  cd conf
  238  ls -l *prop*
  239  vi log4j.properties.template
  240  exit
  241  hdfs dfs -ls /user/w205/lab_3
  242  hdfs dfs -ls /user/w205/lab_3/user_data
  243  hdfs dfs -ls /user/w205/lab_3/weblog_data
  244  hive
  245  pwd
  246  bash ./setup_spark.sh
  247  /data/start_metastore.sh
  248  /data/spark15/bin/spark-sql
  249  hive
  250  pwd
  251  hdfs dfs -ls /user/w205/lab_3/weblog_data
  252  hdfs dfs -ls /user/w205/lab_3
  253  hdfs dfs -ls /user/w205/
  254  hdfs dfs -ls /user/w205/hospital_compare
  255  hdfs dfs -ls /user/w205
  256  hdfs dfs -ls /user/w205/hospitals_compare
  257  pwd
  258  exit
  259  pwd
  260  ls -l
  261  exir
  262  exit
  263  ls -l
  264  hdfs dfs -ls /user/w205/hospital_compare
  265  hdfs dfs -ls /user/w205/weblog*
  266  hdfs dfs -ls /user/w205/weblog_data
  267  hdfs dfs -ls /user/w205/lab_3
  268  hdfs dfs -ls /user/w205/hospital_compare
  269  hdfs dfs -ls /user/w205/hospitals_compare
  270  hdfs dfs -ls /user/w205
  271  pwd
  272  exit
  273  ls -l
  274  pwd
  275  whoami
  276  pwd
  277  ls -l
  278  pwd
  279  ls -l
  280  pwd
  281  hdfs dfs -mkdir /user/w205/exercise_1/hospitals_compare
  282  hdfs dfs -mkdir /user/w205/exercise_1
  283  hdfs dfs -mkdir /user/w205/exercise_1/hospitals_compare
  284  hdfs dfs -mkdir /user/w205/exercise_1/hospital_compare
  285  hdfs dfs -put *.csv /user/w205/exercise_1/hospital_compare/.
  286  hdfs dfs -put *.csv /user/w205/exercise_1/hospitals_compare/.
  287  hdfs dfs -ls /user/w205/exercise_1/hospitals_compare
  288  hdfs dfs -ls /user/w205/exercise_1/hospital_compare
  289  vi /user/w205/exercise_1/hospital_compare/hospitals.csv
  290  ls -l
  291  pwd
  292  vi hive_base_ddl.sql
  293  ls -l
  294  whoami
  295  chown w205 *.csv
  296  su
  297  su - root
  298  ls -l
  299  ls -la
  300  ls -l *.csv
  301  rm hospitals.csv 
  302  rm effective_care.csv 
  303  rm Measures.csv 
  304  rm readmissions.csv 
  305  ls -l
  306  rm surveys_responses.csv 
  307  vi hive_base_ddl.sql 
  308  hive
  309  pwd
  310  hdfs dfs -l /user/w205/hospital_compare
  311  hdfs dfs -ls /user/w205/hospital_compare
  312  history
[w205@ip-172-31-6-204 ~]$ ls -la
total 279904
drwx------  4 w205 w205      4096 Oct  5 00:49 .
drwxr-xr-x  7 root root      4096 Sep 22  2015 ..
-rw-------  1 w205 w205      5474 Oct  4 21:20 .bash_history
-rw-r--r--  1 w205 w205        18 Oct 16  2014 .bash_logout
-rw-r--r--  1 w205 w205       176 Oct 16  2014 .bash_profile
-rw-r--r--  1 w205 w205       124 Oct 16  2014 .bashrc
-rw-r--r--  1 w205 w205       500 May  7  2013 .emacs
-rw-r--r--  1 w205 w205     20480 Oct  5 00:49 .hive_base_ddl.sql.swp
-rw-------  1 w205 w205        16 Sep 28  2015 .psql_history
-rw-------  1 w205 w205      4614 Oct  5 00:44 .viminfo
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 w205 w205      4108 Oct  5 00:49 hive_base_ddl.sql
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ pwd
/home/w205
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/exercise_1/hospital_compare
Found 7 items
-rw-r--r--   1 w205 supergroup      13146 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/Measures.csv
-rw-r--r--   1 w205 supergroup   63280769 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/effective_care.csv
-rw-r--r--   1 w205 supergroup     826758 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/hospitals.csv
-rw-r--r--   1 w205 supergroup   19936145 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/readmissions.csv
-rw-r--r--   1 w205 supergroup    1348499 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/surveys_responses.csv
-rw-r--r--   1 w205 supergroup     166205 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/userdata_lab.csv
-rw-r--r--   1 w205 supergroup    5192992 2016-10-04 22:53 /user/w205/exercise_1/hospital_compare/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ ls -la
total 363420
drwx------  4 w205 w205      4096 Oct  5 01:01 .
drwxr-xr-x  7 root root      4096 Sep 22  2015 ..
-rw-------  1 w205 w205      5474 Oct  4 21:20 .bash_history
-rw-r--r--  1 w205 w205        18 Oct 16  2014 .bash_logout
-rw-r--r--  1 w205 w205       176 Oct 16  2014 .bash_profile
-rw-r--r--  1 w205 w205       124 Oct 16  2014 .bashrc
-rw-r--r--  1 w205 w205       500 May  7  2013 .emacs
-rw-r--r--  1 w205 w205     20480 Oct  5 00:49 .hive_base_ddl.sql.swp
-rw-------  1 w205 w205        16 Sep 28  2015 .psql_history
-rw-------  1 w205 w205      4614 Oct  5 00:44 .viminfo
-rw-r--r--  1 root root     13146 Oct  5 01:00 Measures.csv
-rw-rw-r--  1 w205 w205       693 Sep 28  2015 derby.log
-rw-r--r--  1 root root  63280769 Oct  5 01:01 effective_care.csv
-rw-r--r--  1 w205 w205      4108 Oct  5 00:49 hive_base_ddl.sql
-rw-r--r--  1 root root    826758 Oct  5 01:01 hospitals.csv
-rwxr-xr-x  1 w205 root      1869 Sep 22 17:55 load_data_lake.sh
drwxrwxr-x  5 w205 w205      4096 Sep 28  2015 metastore_db
-rw-r--r--  1 root root  19936145 Oct  5 01:01 readmissions.csv
-rw-rw-r--  1 w205 w205      3666 Jan 25  2016 setup_spark.sh
-rw-rw-r--  1 w205 w205 280869269 Sep  9  2015 spark-1.5.0-bin-hadoop2.6.tgz
drwxr-xr-x 11 w205 w205      4096 Aug 31  2015 spark15
-rw-r--r--  1 root root   1348499 Oct  5 01:01 surveys_responses.csv
-rw-rw-r--  1 w205 w205    166205 Sep 17  2015 userdata_lab.csv
-rw-rw-r--  1 w205 w205   5192992 Sep 17  2015 weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hdfs dfs -put *.csv /user/w205/hospital_compare/.
[w205@ip-172-31-6-204 ~]$ hdfs dfs -ls /user/w205/hospital_compare
Found 7 items
-rw-r--r--   1 w205 supergroup      13146 2016-10-05 01:03 /user/w205/hospital_compare/Measures.csv
-rw-r--r--   1 w205 supergroup   63280769 2016-10-05 01:03 /user/w205/hospital_compare/effective_care.csv
-rw-r--r--   1 w205 supergroup     826758 2016-10-05 01:03 /user/w205/hospital_compare/hospitals.csv
-rw-r--r--   1 w205 supergroup   19936145 2016-10-05 01:03 /user/w205/hospital_compare/readmissions.csv
-rw-r--r--   1 w205 supergroup    1348499 2016-10-05 01:03 /user/w205/hospital_compare/surveys_responses.csv
-rw-r--r--   1 w205 supergroup     166205 2016-10-05 01:03 /user/w205/hospital_compare/userdata_lab.csv
-rw-r--r--   1 w205 supergroup    5192992 2016-10-05 01:03 /user/w205/hospital_compare/weblog_lab.csv
[w205@ip-172-31-6-204 ~]$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/w205/spark15/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.5.jar!/hive-log4j.properties
WARNING: Hive CLI is deprecated and migration to Beeline is recommended.
hive> SELECT * FROM hospitals
    > LIMIT 10;
OK
ACS Participation data	ACS_REGISTRY	3Q2013	2013-07-01 00:00:00	2Q2014	2014-06-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Statin Prescribed at Discharge	AMI_10	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Aspirin Prescribed at Discharge	AMI_2	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Fibrinolytic Therapy Received within 30 Minutes of Hospital Arrival	AMI_7a	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Primary PCI Received Within 90 Minutes of Hospital Arrival	AMI_8a	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Relievers for Inpatient Asthma	CAC_1	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Systemic Corticosteroids for Inpatient Asthma	CAC_2	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Home Management Plan of Care (HMPC) Document Given to Patient/Caregiver	CAC_3	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Complication Rate Following Elective Primary  Total Hip Arthroplasty (THA) and/or Total Knee Arthroplasty (TKA)	COMP_HIP_KNEE	2Q2011	2011-04-01 00:00:00	1Q2014	2014-03-31 00:00:00	NULL	NULL	NULL	NULL	NULL
Median Time from ED Arrival to ED Departure for Admitted ED Patients	ED_1b	4Q2013	2013-10-01 00:00:00	3Q2014	2014-09-30 00:00:00	NULL	NULL	NULL	NULL	NULL
Time taken: 0.921 seconds, Fetched: 10 row(s)
hive> SELECT Provider_ID FROM hospitals, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 100;
FAILED: ParseException line 1:40 missing EOF at '(' near 'COUNT'
hive> SELECT Provider_ID COUNT(Provider_ID) AS hospital_count
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 100;
FAILED: ParseException line 1:24 missing EOF at '(' near 'COUNT'
hive> SELECT Provider_ID, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 100;
Query ID = w205_20161005010808_ffef174d-6f8d-40c6-953f-dac650664b1b
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0003, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0003/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-10-05 01:08:58,022 Stage-1 map = 0%,  reduce = 0%
2016-10-05 01:09:10,113 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.86 sec
2016-10-05 01:09:16,619 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.52 sec
MapReduce Total cumulative CPU time: 10 seconds 520 msec
Ended Job = job_1475607266406_0003
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 10.52 sec   HDFS Read: 90773136 HDFS Write: 1000 SUCCESS
Total MapReduce CPU Time Spent: 10 seconds 520 msec
OK
010001	64
010005	64
010006	64
010007	64
010008	64
010011	64
010012	64
010016	64
010018	63
010019	64
010021	64
010022	64
010023	64
010024	64
010029	64
010032	64
010033	64
010034	64
010035	64
010036	64
010038	64
010039	67
010040	64
010044	64
010045	64
010046	64
010047	64
010049	64
010051	63
010052	64
010054	64
010055	64
010056	64
010058	63
010059	64
010061	64
010062	64
010065	64
010069	64
010073	64
010078	64
010079	64
010083	64
010085	64
010086	64
010087	64
010089	64
010090	64
010091	64
010092	64
010095	63
010097	64
010099	64
010100	64
010101	64
010102	63
010103	64
010104	64
010108	64
010109	64
010110	63
010112	64
010113	64
010114	64
010118	64
010120	64
010125	64
010126	64
010128	63
010129	64
010130	64
010131	64
010138	63
010139	64
010144	64
010146	64
010148	64
010149	64
01014F	27
010150	64
010157	64
010158	64
010164	64
010168	64
010169	64
01019F	27
01021F	27
011300	63
011302	63
011304	62
013301	54
020001	64
020006	64
020008	64
020012	67
020017	64
020018	52
020024	64
020026	64
020027	51
Time taken: 30.031 seconds, Fetched: 100 row(s)
hive> SELECT Provider_ID, Name, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 100;
FAILED: SemanticException [Error 10025]: Line 1:20 Expression not in GROUP BY key 'Name'
hive> SELECT Provider_ID, Name, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals
    > LIMIT 100;
FAILED: SemanticException [Error 10025]: Line 1:7 Expression not in GROUP BY key 'Provider_ID'
hive> SELECT Provider_ID, Name, COUNT(Provider_ID) AS hospital_count
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 100;
FAILED: SemanticException [Error 10025]: Line 1:20 Expression not in GROUP BY key 'Name'
hive> SELECT Provider_ID, Name, COUNT(Provider_ID) AS hospital_count
    > LIMIT 10;
FAILED: SemanticException [Error 10004]: Line 1:32 Invalid table alias or column reference 'Provider_ID': (possible column names are: )
hive> SELECT Provider_ID, Name
    > FROM hospitals GROUP BY Provider_ID
    > LIMIT 10;
FAILED: SemanticException [Error 10025]: Line 1:20 Expression not in GROUP BY key 'Name'
hive> SELECT Provider_ID, Name
    > FROM hospitals
    > LIMIT 100;
OK
ACS Participation data	ACS_REGISTRY
Statin Prescribed at Discharge	AMI_10
Aspirin Prescribed at Discharge	AMI_2
Fibrinolytic Therapy Received within 30 Minutes of Hospital Arrival	AMI_7a
Primary PCI Received Within 90 Minutes of Hospital Arrival	AMI_8a
Relievers for Inpatient Asthma	CAC_1
Systemic Corticosteroids for Inpatient Asthma	CAC_2
Home Management Plan of Care (HMPC) Document Given to Patient/Caregiver	CAC_3
Complication Rate Following Elective Primary  Total Hip Arthroplasty (THA) and/or Total Knee Arthroplasty (TKA)	COMP_HIP_KNEE
Median Time from ED Arrival to ED Departure for Admitted ED Patients	ED_1b
Admit Decision Time to ED Departure Time for Admitted Patients	ED_2b
Emergency Department Volume	EDV
Central Line Associated Bloodstream Infection	HAI_1
Catheter Associated Urinary Tract Infections	HAI_2
SSI - Colon Surgery	HAI_3
SSI - Abdominal Hysterectomy	HAI_4
MRSA Bacteremia	HAI_5
Clostridium Difficile (C.Diff)	HAI_6
Patient satisfaction survey results	HCAHPS
Discharge Instructions	HF_1
Evaluation of LVS Function	HF_2
ACEI or ARB for LVSD	HF_3
Influenza Immunization	IMM_2
Healthcare Personnel Influenza Vaccination	IMM_3
Acute Myocardial Infarction (AMI) 30-Day Mortality Rate	MORT_30_AMI
30-Day All-Cause Mortality Following Coronary Artery Bypass Graft (CABG) Surgery	MORT_30_CABG
Chronic Obstructive Pulmonary Disease (COPD) 30-Day Mortality Rate	MORT_30_COPD
Heart Failure (HF) 30-Day Mortality Rate	MORT_30_HF
Pneumonia 30-Day Mortality Rate	MORT_30_PN
Acute Ischemic Stroke (STK) 30-Day Mortality Rate	MORT_30_STK
Spending per Hospital Patient with Medicare (Medicare Spending per Beneficiary)	MSPB_1
Medicare Volume	MV
Median Time to Fibrinolysis	OP_1
Abdomen CT - Use of Contrast Material	OP_10
Thorax CT - Use of Contrast Material	OP_11
Does/did your facility have the ability to receive laboratory data electronically directly into your ONC certified EHR system as discrete searchable data?	OP_12
Cardiac imaging for preoperative risk assessment for non-cardiac low-risk surgery	OP_13
Simultaneous use of brain Computed Tomography (CT) and sinus Computed Tomography (CT)	OP_14
Does your facility have the ability to track clinical results between visits?	OP_17
Median Time from ED Arrival to ED Departure for Discharged ED Patients	OP_18b
Fibrinolytic Therapy Received Within 30 Minutes of ED Arrival	OP_2
Median Time from ED Arrival to Provider Contact for ED patients	OP_20
Median Time to Pain Management for Long Bone Fracture	OP_21
Patient left without being seen	OP_22
Head CT Scan Results for Acute Ischemic Stroke or Hemorrhagic Stroke Patients who Received Head CT or MRI Scan Interpretation Within 45 Minutes of ED Arrival	OP_23
Safe Surgery Checklist Use	OP_25
Hospital Outpatient Volume Data on Selected Outpatient Surgical Procedures	OP_26
Median Time to Transfer to Another Facility for Acute Coronary Intervention- Reporting Rate	OP_3b
Aspirin at Arrival	OP_4
Median Time to ECG	OP_5
Timing of Antibiotic Prophylaxis	OP_6
Prophylactic Antibiotic Selection for Surgical Patients	OP_7
MRI Lumbar Spine for Low Back Pain	OP_8
Mammography Follow-up Rates	OP_9
Risk-Standardized Payment Associated with a 30-Day AMI Episode-of-Care for Acute Myocardial Infarction	PAYM_30_AMI
Risk-Standardized Payment Associated with a 30-Day Episode of Care for Heart Failure	PAYM_30_HF
Risk-Standardized Payment Associated with a 30-Day Episode of Care for Pneumonia	PAYM_30_PN
Elective Delivery	PC_01
Initial Antibiotic Selection for CAP in Immunocompetent Patient	PN_6
Post-Operative Pulmonary Embolism (PE) or Deep Vein Thrombosis (DVT)	PSI_12
Postoperative wound dehiscence 	PSI_14
Accidental puncture or laceration  	PSI_15
Death among surgical inpatients with serious treatable complications	PSI_4
Iatrogenic pneumothorax, adult	PSI_6
Complication/patient safety for selected indicators (composite)	PSI_90
Acute Myocardial Infarction (AMI) 30-Day Readmission Rate	READM_30_AMI
30-Day All-Cause Unplanned Readmission Following Coronary Artery Bypass Graft Surgery (CABG)	READM_30_CABG
Chronic Obstructive Pulmonary Disease (COPD) 30-Day Readmission Rate	READM_30_COPD
Heart  Failure (HF) 30-Day Readmission Rate	READM_30_HF
30-Day Readmission Rate Following Elective Primary Total Hip Arthroplasty (THA) and/or Total Knee Arthroplasty (TKA)	READM_30_HIP_KNEE
30-Day Hospital-Wide All-Cause Unplanned Readmission Rate	READM_30_HOSP_WIDE
Pneumonia 30-Day Readmission Rate	READM_30_PN
Stroke (STK) 30-Day Readmission Rate	READM_30_STK
Surgery Patients on Beta-Blocker Therapy Prior to Arrival Who Received a Beta-Blocker During the Perioperative Period	SCIP_Card_2
Prophylactic Antibiotic Received Within 1 Hour Prior to Surgical Incision	SCIP_Inf_1
Surgery Patients with Perioperative Temperature Management	SCIP_Inf_10
Prophylactic Antibiotic Selection for Surgical Patients	SCIP_Inf_2
Prophylactic Antibiotics Discontinued Within 24 hours After Surgery End Time	SCIP_Inf_3
Cardiac Surgery Patients with Controlled Postoperative Blood Glucose	SCIP_Inf_4
Urinary  Catheter Removed on Postoperative Day 1 (POD 1) or Postoperative Day 2 (POD 2) with Day of Surgery being Day Zero	SCIP_Inf_9
Surgery Patients Who Received Appropriate Venous Thromboembolism Prophylaxis Within 24 Hours Prior to Surgery to 24 Hours After Surgery	SCIP_VTE_2
Participation in a Systematic Database for Cardiac Surgery	SM_PART_CARD
Participation in a Systematic Clinical Database Registry for General Surgery	SM_PART_GEN_SURG
Participation in a Systematic Clinical Database Registry for Nursing Sensitive Care	SM_PART_NURSE
Participation in a Systematic Clinical Database Registry for Stroke Care	SM_PART_STROKE
HCAHPS Summary Star Rating	Star Rating
Venous Thromboembolism (VTE) Prophylaxis	STK_1
Assessed for Rehabilitation	STK_10
Discharged on Antithrombotic Therapy	STK_2
Anticoagulation Therapy for Atrial Fibrillation/Flutter	STK_3
Thrombolytic Therapy	STK_4
Antithrombotic Therapy By End of Hospital Day 2	STK_5
Discharged on Statin Medication	STK_6
Stroke Education	STK_8
Venous Thromboembolism Prophylaxis	VTE_1
Intensive Care Unit Venous Thromboembolism Prophylaxis	VTE_2
Venous Thromboembolism Patients with Anticoagulation Overlap Therapy	VTE_3
Venous Thromboembolism Patients Receiving Unfractionated Heparin with Dosages/Platelet Count Monitoring by Protocol or Nomogram	VTE_4
Venous Thromboembolism Warfarin Therapy Discharge Instructions	VTE_5
Hospital Acquired Potentially-Preventable Venous Thromboembolism	VTE_6
Time taken: 0.062 seconds, Fetched: 100 row(s)
hive> 
    > 
    > 
    > 
    > DROP TABLE IF EXISTS hospitals;
OK
Time taken: 0.664 seconds
hive> DROP TABLE IF EXISTS hospitals;
OK
Time taken: 0.013 seconds
hive> DROP TABLE IF EXISTS effective_care;
OK
Time taken: 0.118 seconds
hive> DROP TABLE IF EXISTS readmissions;
OK
Time taken: 0.092 seconds
hive> DROP TABLE IF EXISTS measures;
OK
Time taken: 0.084 seconds
hive> DROP TABLE IF EXISTS surveys_responses;
OK
Time taken: 0.1 seconds
hive> 
:wq





















ocalhost:Week4 NatarajanShankar$ vi lab3_log.txt
localhost:Week4 NatarajanShankar$ scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@@ip-172-31-6-204:/home/w205
ssh: Could not resolve hostname ip-172-31-6-204: nodename nor servname provided, or not known
lost connection
localhost:Week4 NatarajanShankar$ whois ip-172-31-6-204

Whois Server Version 2.0

Domain names in the .com and .net domains can now be registered
with many different competing registrars. Go to http://www.internic.net
for detailed information.

No match for "IP-172-31-6-204".
>>> Last update of whois database: Tue, 04 Oct 2016 21:31:14 GMT <<<

NOTICE: The expiration date displayed in this record is the date the
registrar's sponsorship of the domain name registration in the registry is
currently set to expire. This date does not necessarily reflect the expiration
date of the domain name registrant's agreement with the sponsoring
registrar.  Users may consult the sponsoring registrar's Whois database to
view the registrar's reported date of expiration for this registration.

TERMS OF USE: You are not authorized to access or query our Whois
database through the use of electronic processes that are high-volume and
automated except as reasonably necessary to register domain names or
modify existing registrations; the Data in VeriSign Global Registry
Services' ("VeriSign") Whois database is provided by VeriSign for
information purposes only, and to assist persons in obtaining information
about or related to a domain name registration record. VeriSign does not
guarantee its accuracy. By submitting a Whois query, you agree to abide
by the following terms of use: You agree that you may use this Data only
for lawful purposes and that under no circumstances will you use this Data
to: (1) allow, enable, or otherwise support the transmission of mass
unsolicited, commercial advertising or solicitations via e-mail, telephone,
or facsimile; or (2) enable high volume, automated, electronic processes
that apply to VeriSign (or its computer systems). The compilation,
repackaging, dissemination or other use of this Data is expressly
prohibited without the prior written consent of VeriSign. You agree not to
use electronic processes that are automated and high-volume to access or
query the Whois database except as reasonably necessary to register
domain names or modify existing registrations. VeriSign reserves the right
to restrict your access to the Whois database in its sole discretion to ensure
operational stability.  VeriSign may restrict or terminate your access to the
Whois database for failure to abide by these terms of use. VeriSign
reserves the right to modify these terms at any time.

The Registry database contains ONLY .COM, .NET, .EDU domains and
Registrars.
localhost:Week4 NatarajanShankar$ scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ip-172-31-6-204:/home/w205
ssh: Could not resolve hostname ip-172-31-6-204: nodename nor servname provided, or not known
lost connection
localhost:Week4 NatarajanShankar$ history
   55  cd Projects
   56  ls -la
   57  cp natarajan_shankar_p1.ipynb natarajan_shankar_p1.ipynb.bkp3
   58  ls -la
   59  jupyter notebook &
   60  pwd
   61  mkdir ~/junk
   62  cd ~/junk
   63  ls -la
   64  ls
   65  man ls
   66  cd
   67  pwd
   68  ls -la
   69  cd Stanford_2015/
   70  ls -la
   71  cd shankarz.TexST
   72  ls -la
   73  cd R
   74  ls -la
   75  cd ..
   76  ls -la
   77  cd ..
   78  ls -la
   79  cd R
   80  cd shankarz.TexST/
   81  cd R
   82  touch *
   83  ls -l
   84  cd ..
   85  ls -l
   86  cd data
   87  ls -l
   88  cd ..
   89  cd man
   90  ls -l
   91  touch *
   92  cd ..
   93  cd tests
   94  ls -l
   95  cd testthat
   96  ls -l
   97  cat test_TexST.R
   98  :q!
   99  pwd
  100  cd ../../..
  101  pwd
  102  ls -la
  103  git commit shankarz.TexST
  104  su
  105  pwd
  106  cd ../UC_Berkeley/
  107  cd Term2
  108  cd W207
  109  cd Coursework/coursework/
  110  ls -l
  111  cd Week3
  112  ls -l
  113  jupyter notebook &
  114  pwd
  115  ls -la
  116  cd ../Projects
  117  ls -la
  118  cp natarajan_shankar_p1.ipynb natarajan_shankar_p1.ipynb.bkp4
  119  ls -la
  120  history
  121  cd UC_Berkeley/Term2/W205
  122  ls -l
  123  cd Week4
  124  ls -l
  125  cd ../Week3
  126  ls -l
  127  cd ..
  128  ls -l
  129  cd exercise_1
  130  ls -l
  131  cd loading_and_modelling/
  132  ls -l
  133  cd ../..
  134  ls -l
  135  pwd
  136  cd Week4
  137  vi lab4_log.txt
  138  vi lab4_log.txt
  139  vi lab4_log.txt
  140  ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-54-227-71-81.compute-1.amazonaws.com
  141  cd UC_Berkeley/Term2/
  142  ls -l
  143  cd ..
  144  ls -l
  145  cd SQL_work/
  146  ls -l
  147  ls -la
  148  git commit -m "Update all the way up to assignmment 7"
  149  git add SQL
  150  git commit -m "Update all the way up to assignmment 7"
  151  history
  152  git push origin master
  153  su
  154  cd UC_Berkeley//Term2
  155  cd W207
  156  ls -l
  157  cd Coursework/coursework/
  158  ls -la
  159  cd Week3
  160  ls -l
  161  jupyter notebook &
  162  cd ..
  163  pwd
  164  cd ../Projects
  165  cd ..
  166  ls -l
  167  cd ..
  168  ls -l
  169  cd ..
  170  ls -l
  171  cd W207
  172  ls -l
  173  cd Coursework/
  174  ls -l
  175  cd coursework/
  176  ls -l
  177  cd Projects
  178  ls -l
  179  jupyter notebook &
  180  cd UC_Berkeley/
  181  cd Term2
  182  cd W205
  183  cd Week4
  184  ls -l
  185  vi lab4_log.txt 
  186  pwd
  187  vi lab4_log.txt 
  188  pwd
  189  cd ../W207
  190  cd ..
  191  pwd
  192  cd ../W207
  193  ls -l
  194  cd Coursework//coursework/
  195  ls -l
  196  cd Projects
  197  ls -l
  198  jupyter notebook &
  199  ç
  200  ps -aux
  201  ps
  202  kill -9 983
  203  jupyter notebook &
  204  ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-54-227-71-81.compute-1.amazonaws.com
  205  pwd
  206  cd ../../..
  207  pwd
  208  cd ../W205
  209  ls -l
  210  cd Week4
  211  ls -l
  212  mv lab4_log.txt lab3_log.txt
  213  vi lab3_log.txt
  214  exit
  215  cd /Users/NatarajanShankar/UC_Berkeley/Term2/W205/Week4
  216  vi Lab3_Schema_Basic_Queries.txt
  217  ls -la
  218  vi breakout_week4.py
  219  vi breakout_week4_python.py
  220  vi breakout_week4.py
  221  cd ..
  222  pwd
  223  cd ../W207
  224  cd Week1
  225  ls -l
  226  jupyter_notebook &
  227  jupyter notebook &
  228  pwd
  229  cd ../../W205
  230  cd Week4
  231  ls -l
  232  vi Lab3_Schema_Basic_Queries.txt
  233  cal 2016
  234  cd UC_Berkeley/Term2/W205
  235  cd Week4
  236  ls -l
  237  vi lab3_log.txt
  238  vi Lab3_Schema_Basic_Queries.txt
  239  cd UC_Berkeley/Term2/W207/Coursework/coursework/
  240  cd Projects
  241  ls -la
  242  jupyter notebook &
  243  ls-l
  244  cp natarajan_shankar_p1.ipynb natarajan_shankar_p1.ipynb.bkp5
  245  ls-l
  246  ls -l
  247  mv natarajan_shankar_p1_3.ipynb
  248  mv natarajan_shankar_p1_3.ipynb.bkp3 natarajan_shankar_p1_3.ipynb
  249  mv natarajan_shankar_p1.ipynb.bkp3 natarajan_shankar_p1_3.ipynb
  250  mv natarajan_shankar_p1.ipynb.bkp4 natarajan_shankar_p1_4.ipynb
  251  mv natarajan_shankar_p1.ipynb.bkp5 natarajan_shankar_p1_5.ipynb
  252  ls -l
  253  mv natarajan_shankar_p1.ipynb.bkp2 natarajan_shankar_p1_2.ipynb
  254  mv natarajan_shankar_p1.ipynb.bkp1 natarajan_shankar_p1_1.ipynb
  255  ls -l
  256  pwd
  257  ls -la
  258  cp natarajan_shankar_p1.ipynb natarajan_shankar_p1_6.ipynb
  259  cp natarajan_shankar_p1.ipynb natarajan_shankar_p1_7.ipynb
  260  ps
  261  kill -9 2213
  262  cd UC_Berkeley//Term2
  263  cd W207/Coursework
  264  cd coursework/
  265  ls -la
  266  cd Projects
  267  jupyter notebook &
  268  cd UC_Berkeley//Term2
  269  cd W207/Coursework
  270  cd coursework/
  271  cd Projects
  272  jupyter notebook &
  273  pwd
  274  cd ../../..
  275  pwd
  276  cd Week7
  277  ls -l
  278  cd ..
  279  pwd
  280  cd W207
  281  ls -l
  282  cd Coursework/
  283  cd coursework/
  284  ls -l
  285  cd week7
  286  ls -la
  287  jupyter notebook &
  288  jupyter notebook &
  289  jupyter notebook &
  290  pip install theano
  291  cd UC_Berkeley//Term2/W207
  292  cd Coursework//coursework/
  293  cd Week7
  294  ls -l
  295  jupyter notebook &
  296  pip install theano
  297  pip install theano --upgrade
  298  pip2 install theano
  299  cd UC_Berkeley/Term2/W207/Coursework//coursework/Projects/
  300  ls -la
  301  jupyter notebook &
  302  ps
  303  kill -9 384
  304  jupyter notebook &
  305  cd UC_Berkeley/Term2/W207/Coursework//coursework/Proj
  306  cd UC_Berkeley/Term2/W207/Coursework//coursework/Projects/
  307  ls -la
  308  jupyter notebook &
  309  apt-get purge scipy
  310  pip install scipy
  311  pip install scipy --upgrade
  312  pip3 install --upgrade pip
  313  pip3 install jupyter
  314  ps
  315  kill -9 470
  316  jupyter notebook &
  317  cp natarajan_shankar_p1.ipynb natarajan_shankar_p1_8.ipynb
  318  cd UC_Berkeley//Term2/W207/Coursework/coursework//Projects/
  319  ls -la
  320  cd "Untitled Folder"
  321  ls -la
  322  cd ..
  323  rmdir "Untitled Folder"
  324  ls -la
  325  jupyter notebook &
  326  cp natarajan_shankar_p1.ipynb natarajan_shankar_p1_9.ipynb
  327  cd UC_Berkeley//Term2/W207/Coursework/coursework/Projects/
  328  jupyter notebook &
  329  cd UC_Berkeley/Term2/W207/Coursework//coursework/Projects/
  330  ps
  331  kill -9 375
  332  cp natarajan_shankar_p1.ipynb natarajan_shankar_p1_11.ipynb
  333  jupyter notebook &
  334  vi .bashrc
  335  W207
  336  source ./.bashrc
  337  W207
  338  w207
  339  pwd
  340  cd ../Projects
  341  ls -l
  342  jupyter notebook &
  343  ps
  344  kill -9 341
  345  ls -la
  346  jupyter notebook &
  347  ps
  348  kill -9 389
  349  jupyter notebook &
  350  cp natarajan_shankar_p1.ipynb natarajan_shankar_p1_12.ipynb
  351  vi junk
  352  python3
  353  ps
  354  kill -9 998
  355  jupyter notebook &
  356  ls -la
  357  python3
  358  vi junk.py
  359  junk.py
  360  ./junk.py
  361  ls -la
  362  whoami
  363  chmod +x junk.py
  364  ./junk.py
  365  ghjk
  366  vi junk.py
  367  ./junk.py
  368  echo $PROMPT
  369  echo $PATH
  370  vi .cshrc
  371  vi ~/.cshrc
  372  vi ~/.bashrc
  373  python3 ./junk.py
  374  python3
  375  W207
  376  cd UC_Berkeley/
  377  cd Term2
  378  cd W207
  379  cd Coursework/
  380  cd coursework/
  381  cd Proj
  382  cd Projects/
  383  ls -la
  384  jupyter notebook &
  385  ps
  386  kill -9 410
  387  pwd
  388  cal 2016
  389  cd UC_Berkeley/Term2/W205
  390  ls -la
  391  ls -la
  392  cd ..
  393  ls -la
  394  cd W205_HW
  395  ls -la
  396  cd MIDS-W205_A1/
  397  ls -la
  398  cd exercise_1
  399  mkdir transforming
  400  ls -la
  401  W207
  402  source ~/.bashrc
  403  W207
  404  w207
  405  pwd
  406  cd ../Projects
  407  ls -la
  408  jupyter notebook &
  409  pwd
  410  cd ../Week7
  411  ls -la
  412  jupyter notebook &
  413  cd UC_Berkeley/Term2/
  414  ls -la
  415  cd W205_HW
  416  l -l
  417  ls -l
  418  cd MIDS-W205_A1/
  419  ls -l
  420  cd exercise_1/
  421  ls -l
  422  cd loading_and_modelling/
  423  ls -l
  424  pwd
  425  ls -l
  426  cd ..
  427  ls -l
  428  cd ..
  429  ls -l
  430  cd ..
  431  ls -l
  432  cd ..
  433  pwd
  434  cd W205
  435  ls -l
  436  cd Week4
  437  ls -l
  438  vi lab3_log.txt
  439  pwd
  440  cd ..
  441  ls -la
  442  cd week4
  443  ls -l
  444  cd ../week3
  445  ls -l
  446  cd Hospital_Revised_Flatfiles
  447  ls -l
  448  cd test
  449  ls -l
  450  cd ..
  451  pwd
  452  ls -l
  453  cd ..
  454  ls -l
  455  ./load_data_lake.sh
  456  ls -l
  457  vi load_data_lake.sh
  458  ls -l
  459  pwd
  460  cd ../Week4
  461  ls -l
  462  cd ../wee3/hospitals.csv .
  463  cd ../week3/hospitals.csv .
  464  cp ../week3/hospitals.csv .
  465  cp ../week3/readmissions.csv .
  466  cp ../week3/Measures.csv .
  467  cp ../week3/surveys_responses.csv .
  468  cp ../week3/effective_care.csv .
  469  ls -l
  470  cd ../week3
  471  ls -l
  472  cd ..
  473  ls -l
  474  cd week4
  475  pwd
  476  ls -l
  477  scp -i “../ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospital_compare/.
  478  exit
  479  quit
  480  quit()
  481  su
  482  cd UC_Berkeley//Term2
  483  ls -l
  484  cd W205/week4
  485  ls -l
  486  scp -i “../ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospital_compare/.
  487  cd UC_Berkeley/
  488  ls -l
  489  cd Term2
  490  ls -l
  491  cd w205
  492  ls -l
  493  scp -i “../ucb3.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospital_compare/.
  494  exit
  495  ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-54-227-71-81.compute-1.amazonaws.com
  496  ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-107-20-151-185.compute-1.amazonaws.com
  497  scp -i “../ucb3.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospital_compare/.
  498  history
  499  scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospital_compare/.
  500  scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospital_compare/.
  501  pwd
  502  cd UC_Berkeley/Term2/W205/Week4
  503  ls -la
  504  scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospital_compare/.
  505  scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/root/.
  506  scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:
  507  pwd
  508  ls -l
  509  cd ..
  510  ls -l
  511  chmod 444  ucb5.pem
  512  cd week4
  513  scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/root/.
  514  pwd
  515  ls -l
  516  cd ../week3
  517  ls -l
  518  ls -l *.txt
  519  cd ..
  520  ls -l *.txt
  521  pwd
  522  cd week4
  523  scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:.
  524  ls -l /Users/NatarajanShankar/UC_Berkeley/Term2/W205/
  525  cd ../..
  526  pwd
  527  ls -l
  528  cd W205_HW
  529  ls -l
  530  cd MIDS-W205_A1/
  531  ls -l
  532  cd exercise_1/
  533  ls -l
  534  cd load*
  535  ls -l
  536  cd ../../..
  537  pwd
  538  cd ../W205
  539  ls -l
  540  cd Week3
  541  ls -l
  542  cd test
  543  ls -l
  544  ls -l *.txt
  545  cd hospitals_compare
  546  ls -l
  547  pwd
  548  cd ../../../Week4
  549  ls -l
  550  vi lab3_log.txt
  551  scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@@ip-172-31-6-204:/home/w205
  552  whois ip-172-31-6-204
  553  scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ip-172-31-6-204:/home/w205
  554  history
localhost:Week4 NatarajanShankar$ scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ip-172-31-6-204:/home/w205.compute-1.amazonaws.com:/home/w205/.
ssh: Could not resolve hostname ip-172-31-6-204: nodename nor servname provided, or not known
lost connection
localhost:Week4 NatarajanShankar$ scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ip-172-31-6-204.compute-1.amazonaws.com:/home/w205/.
ssh: Could not resolve hostname ip-172-31-6-204.compute-1.amazonaws.com: nodename nor servname provided, or not known
lost connection
localhost:Week4 NatarajanShankar$ !504
scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospital_compare/.
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205/Week4
localhost:Week4 NatarajanShankar$ scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/root/.
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/root/.
Password:
The authenticity of host 'ec2-107-20-151-185.compute-1.amazonaws.com (107.20.151.185)' can't be established.
RSA key fingerprint is a8:5d:e8:8d:35:d6:cf:9c:d5:64:45:cd:8b:53:c0:a8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ec2-107-20-151-185.compute-1.amazonaws.com,107.20.151.185' (RSA) to the list of known hosts.
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/user/w205/hospitals_compare
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/user/w205/hospitals_compare
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/user/w205
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/root/.
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ su
Password:
sh-3.2# pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205/Week4
sh-3.2# sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/root/.
Permission denied (publickey).
lost connection
sh-3.2# ls -l /Users/NatarajanShankar/UC_Berkeley/Term2/W205/
total 1472
-rw-r--r--@  1 NatarajanShankar  staff   15364 Sep 28 07:12 .DS_Store
drwxr-xr-x  15 NatarajanShankar  staff     510 Sep 26 11:43 .git
-rw-r--r--@  1 NatarajanShankar  staff  164720 Sep  5 22:20 EBS_vs_S3.docx
-rw-r--r--@  1 NatarajanShankar  staff  107009 Sep 17 13:59 Git_commit_commands.docx
-rw-r--r--@  1 NatarajanShankar  staff  279701 Sep 12 14:15 MIDS W205 Syllabus-v2-2.pdf
-rw-r--r--@  1 NatarajanShankar  staff  139776 Oct  3 22:41 Project_1.ppt
-rw-r--r--   1 NatarajanShankar  staff      26 Sep 17 13:36 README.md
drwxr-xr-x  11 NatarajanShankar  staff     374 Sep 17 13:41 Week1
drwxr-xr-x  16 NatarajanShankar  staff     544 Sep 17 13:52 Week2
drwxr-xr-x  70 NatarajanShankar  staff    2380 Oct  4 12:42 Week3
drwxr-xr-x  13 NatarajanShankar  staff     442 Oct  4 14:30 Week4
drwxr-xr-x   3 NatarajanShankar  staff     102 Sep 27 08:45 Week5
drwxr-xr-x   4 NatarajanShankar  staff     136 Sep 22 11:12 exercise_1
-rwxr-xr-x   1 NatarajanShankar  staff    5716 Oct  9  2015 setup_ucb_complete_plus_postgres.sh
-rw-r--r--@  1 NatarajanShankar  staff    1692 Sep  4 21:28 ucb1.pem.next.old.pem
-r--------@  1 NatarajanShankar  staff    1696 Sep  3 22:01 ucb1.pem.old .pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 20 11:04 ucb3.pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 12 09:42 ucb3.pem.latest
-r--r--r--@  1 NatarajanShankar  staff    1696 Sep 20 17:26 ucb5.pem
sh-3.2# pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205/Week4
sh-3.2# sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/users/w205
Permission denied (publickey).
lost connection
sh-3.2# sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/users/w205/.
Permission denied (publickey).
lost connection
sh-3.2# sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/users/w205/.
Permission denied (publickey).
lost connection
sh-3.2# pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205/Week4
sh-3.2# ls -la
total 169432
drwxr-xr-x  13 NatarajanShankar  staff       442 Oct  4 14:30 .
drwxr-xr-x  21 NatarajanShankar  staff       714 Oct  3 21:40 ..
-rw-r--r--@  1 NatarajanShankar  staff      6148 Sep 28 07:12 .DS_Store
-rw-r--r--   1 NatarajanShankar  staff       963 Sep 27 22:13 Lab3_Schema_Basic_Queries.txt
-rw-r--r--   1 NatarajanShankar  staff     13146 Oct  4 12:44 Measures.csv
-rw-r--r--   1 NatarajanShankar  staff      3520 Sep 21 17:13 breakout_week4.py
-rw-r--r--   1 NatarajanShankar  staff      3520 Sep 21 17:19 breakout_week4_python.py
-rw-r--r--   1 NatarajanShankar  staff  63280769 Oct  4 12:44 effective_care.csv
-rw-r--r--   1 NatarajanShankar  staff    826758 Oct  4 12:43 hospitals.csv
-rw-r--r--   1 NatarajanShankar  staff     47492 Sep 27 09:14 lab3_log.txt
-rw-r--r--@  1 NatarajanShankar  staff   1260848 Sep 21 17:45 live_session_week_4_subset.pdf
-rw-r--r--   1 NatarajanShankar  staff  19936145 Oct  4 12:44 readmissions.csv
-rw-r--r--   1 NatarajanShankar  staff   1348499 Oct  4 12:44 surveys_responses.csv
sh-3.2# chmod 666 *.csv
sh-3.2# ls -l
total 169432
-rw-r--r--@ 1 NatarajanShankar  staff      6148 Sep 28 07:12 .DS_Store
-rw-r--r--  1 NatarajanShankar  staff       963 Sep 27 22:13 Lab3_Schema_Basic_Queries.txt
-rw-rw-rw-  1 NatarajanShankar  staff     13146 Oct  4 12:44 Measures.csv
-rw-r--r--  1 NatarajanShankar  staff      3520 Sep 21 17:13 breakout_week4.py
-rw-r--r--  1 NatarajanShankar  staff      3520 Sep 21 17:19 breakout_week4_python.py
-rw-rw-rw-  1 NatarajanShankar  staff  63280769 Oct  4 12:44 effective_care.csv
-rw-rw-rw-  1 NatarajanShankar  staff    826758 Oct  4 12:43 hospitals.csv
-rw-r--r--  1 NatarajanShankar  staff     47492 Sep 27 09:14 lab3_log.txt
-rw-r--r--@ 1 NatarajanShankar  staff   1260848 Sep 21 17:45 live_session_week_4_subset.pdf
-rw-rw-rw-  1 NatarajanShankar  staff  19936145 Oct  4 12:44 readmissions.csv
-rw-rw-rw-  1 NatarajanShankar  staff   1348499 Oct  4 12:44 surveys_responses.csv
sh-3.2# sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/users/w205/.
Permission denied (publickey).
lost connection
sh-3.2# !ssh
sh: !ssh: event not found
sh-3.2# exit
exit
localhost:Week4 NatarajanShankar$ !ssh
ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-107-20-151-185.compute-1.amazonaws.com
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Permissions 0444 for '/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem' are too open.
It is required that your private key files are NOT accessible by others.
This private key will be ignored.
bad permissions: ignore key: /Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem
Permission denied (publickey).
localhost:Week4 NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205/Week4
localhost:Week4 NatarajanShankar$ chmod 400 ucb.pem
chmod: ucb.pem: No such file or directory
localhost:Week4 NatarajanShankar$ chmod 400 ../ucb.pem
chmod: ../ucb.pem: No such file or directory
localhost:Week4 NatarajanShankar$ chmod 400 ../ucb5.pem
localhost:Week4 NatarajanShankar$ ssh -i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" root@ec2-107-20-151-185.compute-1.amazonaws.com
Last login: Tue Oct  4 21:13:55 2016 from 73.223.185.251
     ___   _        __   __   ____            __    
    / _ \ (_)___ _ / /  / /_ / __/____ ___ _ / /___ 
   / , _// // _ `// _ \/ __/_\ \ / __// _ `// // -_)
  /_/|_|/_/ \_, //_//_/\__//___/ \__/ \_,_//_/ \__/ 
           /___/                                                 
                                              
Welcome to a virtual machine image brought to you by RightScale!


[root@ip-172-31-6-204 ~]# exit
logout

Connection to ec2-107-20-151-185.compute-1.amazonaws.com closed.
localhost:Week4 NatarajanShankar$ su
Password:
sh-3.2# sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/.
Permission denied (publickey).
lost connection
sh-3.2# sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/.
Permission denied (publickey).
lost connection
sh-3.2# exit
exit
localhost:Week4 NatarajanShankar$ sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospitals_compare
Password:
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospitals_compare
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@107.20.151.185:/home/w205/hospitals_compare
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ ^scp^
sudo  - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@107.20.151.185:/home/w205/hospitals_compare
sudo: -: command not found
localhost:Week4 NatarajanShankar$ sudo scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@107.20.151.185:/home/w205/hospitals_compare

Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ 
localhost:Week4 NatarajanShankar$ scp - i "/Users/NatarajanShankar/UC_Berkeley/Term2/W205/ucb5.pem" *.csv w205@107.20.151.185:/home/w205/hospitals_compare
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ cd ..
localhost:W205 NatarajanShankar$ scp - i ucb5.pem */week4/.csv w205@107.20.151.185:/home/w205/hospitals_compare
Permission denied (publickey).
lost connection
localhost:W205 NatarajanShankar$ scp  ucb5.pem */week4/.csv w205@107.20.151.185:/home/w205/hospitals_compare
Permission denied (publickey).
lost connection
localhost:W205 NatarajanShankar$ man scp
localhost:W205 NatarajanShankar$ 
localhost:W205 NatarajanShankar$ cd ..
localhost:Term2 NatarajanShankar$ ls -l
total 8
drwxr-xr-x  21 NatarajanShankar  staff   714 Oct  3 21:40 W205
drwxr-xr-x   4 NatarajanShankar  staff   136 Sep 24 20:45 W205_HW
drwxr-xr-x   4 NatarajanShankar  staff   136 Sep 24 11:34 W205_Uri
drwxr-xr-x   9 NatarajanShankar  staff   306 Sep  9 19:39 W207
-r--------@  1 NatarajanShankar  staff  1692 Sep  6 09:02 ucb3.pem.latest
localhost:Term2 NatarajanShankar$ cd ..
localhost:UC_Berkeley NatarajanShankar$ ls -l
total 512
-rw-r--r--@  1 NatarajanShankar  staff  156728 Dec 16  2015 Berkeley Statement of Purpose.docx
drwxr-xr-x  13 NatarajanShankar  staff     442 Aug 29 11:36 Python Training
drwxr-xr-x   6 NatarajanShankar  staff     204 Sep 10 18:52 SQL_work
-rw-r--r--@  1 NatarajanShankar  staff  101832 Jan 15  2016 Shankar GRE 2016.pdf
drwxr-xr-x   4 NatarajanShankar  staff     136 Jul 15 08:35 Tableau
drwxr-xr-x   5 NatarajanShankar  staff     170 Sep  1 11:09 Term1
drwxr-xr-x   8 NatarajanShankar  staff     272 Sep 24 20:20 Term2
localhost:UC_Berkeley NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley
localhost:UC_Berkeley NatarajanShankar$ cd Term2
localhost:Term2 NatarajanShankar$ ls -l
total 8
drwxr-xr-x  21 NatarajanShankar  staff   714 Oct  3 21:40 W205
drwxr-xr-x   4 NatarajanShankar  staff   136 Sep 24 20:45 W205_HW
drwxr-xr-x   4 NatarajanShankar  staff   136 Sep 24 11:34 W205_Uri
drwxr-xr-x   9 NatarajanShankar  staff   306 Sep  9 19:39 W207
-r--------@  1 NatarajanShankar  staff  1692 Sep  6 09:02 ucb3.pem.latest
localhost:Term2 NatarajanShankar$ cd W2017
-bash: cd: W2017: No such file or directory
localhost:Term2 NatarajanShankar$ cd W207
localhost:W207 NatarajanShankar$ l s-l
-bash: l: command not found
localhost:W207 NatarajanShankar$ ls -l
total 0
drwxr-xr-x  5 NatarajanShankar  staff  170 Sep  8 16:49 Coursework
drwxr-xr-x  7 NatarajanShankar  staff  238 Sep 25 23:03 Week1
drwxr-xr-x  8 NatarajanShankar  staff  272 Sep  9 19:34 Week2
drwxr-xr-x  5 NatarajanShankar  staff  170 Sep 17 13:02 Week3
drwxr-xr-x  8 NatarajanShankar  staff  272 Oct  3 23:45 Week4
drwxr-xr-x  7 NatarajanShankar  staff  238 Oct  3 23:48 Week5
localhost:W207 NatarajanShankar$ cd .../W205
-bash: cd: .../W205: No such file or directory
localhost:W207 NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W207
localhost:W207 NatarajanShankar$ cd ../W205
localhost:W205 NatarajanShankar$ ls -l
total 1440
-rw-r--r--@  1 NatarajanShankar  staff  164720 Sep  5 22:20 EBS_vs_S3.docx
-rw-r--r--@  1 NatarajanShankar  staff  107009 Sep 17 13:59 Git_commit_commands.docx
-rw-r--r--@  1 NatarajanShankar  staff  279701 Sep 12 14:15 MIDS W205 Syllabus-v2-2.pdf
-rw-r--r--@  1 NatarajanShankar  staff  139776 Oct  3 22:41 Project_1.ppt
-rw-r--r--   1 NatarajanShankar  staff      26 Sep 17 13:36 README.md
drwxr-xr-x  11 NatarajanShankar  staff     374 Sep 17 13:41 Week1
drwxr-xr-x  16 NatarajanShankar  staff     544 Sep 17 13:52 Week2
drwxr-xr-x  70 NatarajanShankar  staff    2380 Oct  4 12:42 Week3
drwxr-xr-x  13 NatarajanShankar  staff     442 Oct  4 14:30 Week4
drwxr-xr-x   3 NatarajanShankar  staff     102 Sep 27 08:45 Week5
drwxr-xr-x   4 NatarajanShankar  staff     136 Sep 22 11:12 exercise_1
-rwxr-xr-x   1 NatarajanShankar  staff    5716 Oct  9  2015 setup_ucb_complete_plus_postgres.sh
-rw-r--r--@  1 NatarajanShankar  staff    1692 Sep  4 21:28 ucb1.pem.next.old.pem
-r--------@  1 NatarajanShankar  staff    1696 Sep  3 22:01 ucb1.pem.old .pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 20 11:04 ucb3.pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 12 09:42 ucb3.pem.latest
-r--------@  1 NatarajanShankar  staff    1696 Sep 20 17:26 ucb5.pem
localhost:W205 NatarajanShankar$ cd W205
-bash: cd: W205: No such file or directory
localhost:W205 NatarajanShankar$ ls -l
total 1440
-rw-r--r--@  1 NatarajanShankar  staff  164720 Sep  5 22:20 EBS_vs_S3.docx
-rw-r--r--@  1 NatarajanShankar  staff  107009 Sep 17 13:59 Git_commit_commands.docx
-rw-r--r--@  1 NatarajanShankar  staff  279701 Sep 12 14:15 MIDS W205 Syllabus-v2-2.pdf
-rw-r--r--@  1 NatarajanShankar  staff  139776 Oct  3 22:41 Project_1.ppt
-rw-r--r--   1 NatarajanShankar  staff      26 Sep 17 13:36 README.md
drwxr-xr-x  11 NatarajanShankar  staff     374 Sep 17 13:41 Week1
drwxr-xr-x  16 NatarajanShankar  staff     544 Sep 17 13:52 Week2
drwxr-xr-x  70 NatarajanShankar  staff    2380 Oct  4 12:42 Week3
drwxr-xr-x  13 NatarajanShankar  staff     442 Oct  4 14:30 Week4
drwxr-xr-x   3 NatarajanShankar  staff     102 Sep 27 08:45 Week5
drwxr-xr-x   4 NatarajanShankar  staff     136 Sep 22 11:12 exercise_1
-rwxr-xr-x   1 NatarajanShankar  staff    5716 Oct  9  2015 setup_ucb_complete_plus_postgres.sh
-rw-r--r--@  1 NatarajanShankar  staff    1692 Sep  4 21:28 ucb1.pem.next.old.pem
-r--------@  1 NatarajanShankar  staff    1696 Sep  3 22:01 ucb1.pem.old .pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 20 11:04 ucb3.pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 12 09:42 ucb3.pem.latest
-r--------@  1 NatarajanShankar  staff    1696 Sep 20 17:26 ucb5.pem
localhost:W205 NatarajanShankar$ cd Week3
localhost:Week3 NatarajanShankar$ ls -l *.txt
-rw-r--r--  1 NatarajanShankar  staff  352 May 12  2005 readme.txt
localhost:Week3 NatarajanShankar$ cd ../week4
localhost:week4 NatarajanShankar$ ls -l
total 169416
-rw-r--r--  1 NatarajanShankar  staff       963 Sep 27 22:13 Lab3_Schema_Basic_Queries.txt
-rw-rw-rw-  1 NatarajanShankar  staff     13146 Oct  4 12:44 Measures.csv
-rw-r--r--  1 NatarajanShankar  staff      3520 Sep 21 17:13 breakout_week4.py
-rw-r--r--  1 NatarajanShankar  staff      3520 Sep 21 17:19 breakout_week4_python.py
-rw-rw-rw-  1 NatarajanShankar  staff  63280769 Oct  4 12:44 effective_care.csv
-rw-rw-rw-  1 NatarajanShankar  staff    826758 Oct  4 12:43 hospitals.csv
-rw-r--r--  1 NatarajanShankar  staff     47492 Sep 27 09:14 lab3_log.txt
-rw-r--r--@ 1 NatarajanShankar  staff   1260848 Sep 21 17:45 live_session_week_4_subset.pdf
-rw-rw-rw-  1 NatarajanShankar  staff  19936145 Oct  4 12:44 readmissions.csv
-rw-rw-rw-  1 NatarajanShankar  staff   1348499 Oct  4 12:44 surveys_responses.csv
localhost:week4 NatarajanShankar$ fgrep scp *.txt
localhost:week4 NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205/week4
localhost:week4 NatarajanShankar$ cd ..
localhost:W205 NatarajanShankar$ ls -l
total 1440
-rw-r--r--@  1 NatarajanShankar  staff  164720 Sep  5 22:20 EBS_vs_S3.docx
-rw-r--r--@  1 NatarajanShankar  staff  107009 Sep 17 13:59 Git_commit_commands.docx
-rw-r--r--@  1 NatarajanShankar  staff  279701 Sep 12 14:15 MIDS W205 Syllabus-v2-2.pdf
-rw-r--r--@  1 NatarajanShankar  staff  139776 Oct  3 22:41 Project_1.ppt
-rw-r--r--   1 NatarajanShankar  staff      26 Sep 17 13:36 README.md
drwxr-xr-x  11 NatarajanShankar  staff     374 Sep 17 13:41 Week1
drwxr-xr-x  16 NatarajanShankar  staff     544 Sep 17 13:52 Week2
drwxr-xr-x  70 NatarajanShankar  staff    2380 Oct  4 12:42 Week3
drwxr-xr-x  13 NatarajanShankar  staff     442 Oct  4 14:30 Week4
drwxr-xr-x   3 NatarajanShankar  staff     102 Sep 27 08:45 Week5
drwxr-xr-x   4 NatarajanShankar  staff     136 Sep 22 11:12 exercise_1
-rwxr-xr-x   1 NatarajanShankar  staff    5716 Oct  9  2015 setup_ucb_complete_plus_postgres.sh
-rw-r--r--@  1 NatarajanShankar  staff    1692 Sep  4 21:28 ucb1.pem.next.old.pem
-r--------@  1 NatarajanShankar  staff    1696 Sep  3 22:01 ucb1.pem.old .pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 20 11:04 ucb3.pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 12 09:42 ucb3.pem.latest
-r--------@  1 NatarajanShankar  staff    1696 Sep 20 17:26 ucb5.pem
localhost:W205 NatarajanShankar$ scp -i ucb5.pem ./Week4/*.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/hospital_compare/.
scp: /home/w205/hospital_compare/.: No such file or directory
localhost:W205 NatarajanShankar$ scp -i ucb5.pem ./Week4/*.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/.
Measures.csv                                                                                                                          100%   13KB  12.8KB/s   00:00    
effective_care.csv                                                                                                                    100%   60MB   1.3MB/s   00:46    
hospitals.csv                                                                                                                         100%  807KB 807.4KB/s   00:00    
readmissions.csv                                                                                                                      100%   19MB   1.6MB/s   00:12    
surveys_responses.csv                                                                                                                 100% 1317KB   1.3MB/s   00:00    
localhost:W205 NatarajanShankar$ 
localhost:W205 NatarajanShankar$ 
localhost:W205 NatarajanShankar$ ls -l
total 1440
-rw-r--r--@  1 NatarajanShankar  staff  164720 Sep  5 22:20 EBS_vs_S3.docx
-rw-r--r--@  1 NatarajanShankar  staff  107009 Sep 17 13:59 Git_commit_commands.docx
-rw-r--r--@  1 NatarajanShankar  staff  279701 Sep 12 14:15 MIDS W205 Syllabus-v2-2.pdf
-rw-r--r--@  1 NatarajanShankar  staff  139776 Oct  3 22:41 Project_1.ppt
-rw-r--r--   1 NatarajanShankar  staff      26 Sep 17 13:36 README.md
drwxr-xr-x  11 NatarajanShankar  staff     374 Sep 17 13:41 Week1
drwxr-xr-x  16 NatarajanShankar  staff     544 Sep 17 13:52 Week2
drwxr-xr-x  70 NatarajanShankar  staff    2380 Oct  4 12:42 Week3
drwxr-xr-x  13 NatarajanShankar  staff     442 Oct  4 14:30 Week4
drwxr-xr-x   3 NatarajanShankar  staff     102 Sep 27 08:45 Week5
drwxr-xr-x   4 NatarajanShankar  staff     136 Sep 22 11:12 exercise_1
-rwxr-xr-x   1 NatarajanShankar  staff    5716 Oct  9  2015 setup_ucb_complete_plus_postgres.sh
-rw-r--r--@  1 NatarajanShankar  staff    1692 Sep  4 21:28 ucb1.pem.next.old.pem
-r--------@  1 NatarajanShankar  staff    1696 Sep  3 22:01 ucb1.pem.old .pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 20 11:04 ucb3.pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 12 09:42 ucb3.pem.latest
-r--------@  1 NatarajanShankar  staff    1696 Sep 20 17:26 ucb5.pem
localhost:W205 NatarajanShankar$ cd Week4
localhost:Week4 NatarajanShankar$ vi hospitals.com
localhost:Week4 NatarajanShankar$ vi hospitals.csv
localhost:Week4 NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205/Week4
localhost:Week4 NatarajanShankar$ scp -i ucb5.pem ./Week4/*.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/.
Warning: Identity file ucb5.pem not accessible: No such file or directory.
Permission denied (publickey).
lost connection
localhost:Week4 NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205/Week4
localhost:Week4 NatarajanShankar$ cd ..
localhost:W205 NatarajanShankar$ scp -i ucb5.pem ./Week4/*.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/.
Permission denied (publickey).
lost connection
localhost:W205 NatarajanShankar$ sudo scp -i ucb5.pem ./Week4/*.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/.
Password:
Permission denied (publickey).
lost connection
localhost:W205 NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205
localhost:W205 NatarajanShankar$ ls -l
total 1440
-rw-r--r--@  1 NatarajanShankar  staff  164720 Sep  5 22:20 EBS_vs_S3.docx
-rw-r--r--@  1 NatarajanShankar  staff  107009 Sep 17 13:59 Git_commit_commands.docx
-rw-r--r--@  1 NatarajanShankar  staff  279701 Sep 12 14:15 MIDS W205 Syllabus-v2-2.pdf
-rw-r--r--@  1 NatarajanShankar  staff  139776 Oct  3 22:41 Project_1.ppt
-rw-r--r--   1 NatarajanShankar  staff      26 Sep 17 13:36 README.md
drwxr-xr-x  11 NatarajanShankar  staff     374 Sep 17 13:41 Week1
drwxr-xr-x  16 NatarajanShankar  staff     544 Sep 17 13:52 Week2
drwxr-xr-x  70 NatarajanShankar  staff    2380 Oct  4 12:42 Week3
drwxr-xr-x  13 NatarajanShankar  staff     442 Oct  4 17:28 Week4
drwxr-xr-x   3 NatarajanShankar  staff     102 Sep 27 08:45 Week5
drwxr-xr-x   4 NatarajanShankar  staff     136 Sep 22 11:12 exercise_1
-rwxr-xr-x   1 NatarajanShankar  staff    5716 Oct  9  2015 setup_ucb_complete_plus_postgres.sh
-rw-r--r--@  1 NatarajanShankar  staff    1692 Sep  4 21:28 ucb1.pem.next.old.pem
-r--------@  1 NatarajanShankar  staff    1696 Sep  3 22:01 ucb1.pem.old .pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 20 11:04 ucb3.pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 12 09:42 ucb3.pem.latest
-r--------@  1 NatarajanShankar  staff    1696 Sep 20 17:26 ucb5.pem
localhost:W205 NatarajanShankar$ sudo scp -i ucb5.pem ./Week4/*.csv w205@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/.
Permission denied (publickey).
lost connection
localhost:W205 NatarajanShankar$ sudo scp -i ucb5.pem ./Week4/*.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/.
Measures.csv                                                                                                                          100%   13KB  12.8KB/s   00:00    
effective_care.csv                                                                                                                    100%   60MB   1.3MB/s   00:46    
hospitals.csv                                                                                                                         100%  807KB 807.4KB/s   00:00    
readmissions.csv                                                                                                                      100%   19MB   1.5MB/s   00:13    
surveys_responses.csv                                                                                                                 100% 1317KB   1.3MB/s   00:00    
localhost:W205 NatarajanShankar$ ls -l
total 1440
-rw-r--r--@  1 NatarajanShankar  staff  164720 Sep  5 22:20 EBS_vs_S3.docx
-rw-r--r--@  1 NatarajanShankar  staff  107009 Sep 17 13:59 Git_commit_commands.docx
-rw-r--r--@  1 NatarajanShankar  staff  279701 Sep 12 14:15 MIDS W205 Syllabus-v2-2.pdf
-rw-r--r--@  1 NatarajanShankar  staff  139776 Oct  3 22:41 Project_1.ppt
-rw-r--r--   1 NatarajanShankar  staff      26 Sep 17 13:36 README.md
drwxr-xr-x  11 NatarajanShankar  staff     374 Sep 17 13:41 Week1
drwxr-xr-x  16 NatarajanShankar  staff     544 Sep 17 13:52 Week2
drwxr-xr-x  70 NatarajanShankar  staff    2380 Oct  4 12:42 Week3
drwxr-xr-x  13 NatarajanShankar  staff     442 Oct  4 17:28 Week4
drwxr-xr-x   3 NatarajanShankar  staff     102 Sep 27 08:45 Week5
drwxr-xr-x   4 NatarajanShankar  staff     136 Sep 22 11:12 exercise_1
-rwxr-xr-x   1 NatarajanShankar  staff    5716 Oct  9  2015 setup_ucb_complete_plus_postgres.sh
-rw-r--r--@  1 NatarajanShankar  staff    1692 Sep  4 21:28 ucb1.pem.next.old.pem
-r--------@  1 NatarajanShankar  staff    1696 Sep  3 22:01 ucb1.pem.old .pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 20 11:04 ucb3.pem
-r--------@  1 NatarajanShankar  staff    1692 Sep 12 09:42 ucb3.pem.latest
-r--------@  1 NatarajanShankar  staff    1696 Sep 20 17:26 ucb5.pem
localhost:W205 NatarajanShankar$ cd Week4
localhost:Week4 NatarajanShankar$ ls -l
total 169416
-rw-r--r--  1 NatarajanShankar  staff       963 Sep 27 22:13 Lab3_Schema_Basic_Queries.txt
-rw-rw-rw-  1 NatarajanShankar  staff     13146 Oct  4 12:44 Measures.csv
-rw-r--r--  1 NatarajanShankar  staff      3520 Sep 21 17:13 breakout_week4.py
-rw-r--r--  1 NatarajanShankar  staff      3520 Sep 21 17:19 breakout_week4_python.py
-rw-rw-rw-  1 NatarajanShankar  staff  63280769 Oct  4 12:44 effective_care.csv
-rw-rw-rw-  1 NatarajanShankar  staff    826758 Oct  4 12:43 hospitals.csv
-rw-r--r--  1 NatarajanShankar  staff     47492 Sep 27 09:14 lab3_log.txt
-rw-r--r--@ 1 NatarajanShankar  staff   1260848 Sep 21 17:45 live_session_week_4_subset.pdf
-rw-rw-rw-  1 NatarajanShankar  staff  19936145 Oct  4 12:44 readmissions.csv
-rw-rw-rw-  1 NatarajanShankar  staff   1348499 Oct  4 12:44 surveys_responses.csv
localhost:Week4 NatarajanShankar$ whoami
NatarajanShankar
localhost:Week4 NatarajanShankar$ vi Measures.csv
localhost:Week4 NatarajanShankar$ vi readmissions
localhost:Week4 NatarajanShankar$ !!.csv
vi readmissions.csv
localhost:Week4 NatarajanShankar$ vi effective_care.csv 
localhost:Week4 NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205/Week4
localhost:Week4 NatarajanShankar$ cd ..
localhost:W205 NatarajanShankar$ sudo scp -i ucb5.pem ./Week4/*.csv root@ec2-107-20-151-185.compute-1.amazonaws.com:/home/w205/.
Password:
Measures.csv                                                                                                                          100%   13KB  12.8KB/s   00:00    
effective_care.csv                                                                                                                    100%   60MB   1.4MB/s   00:42    
hospitals.csv                                                                                                                         100%  807KB 807.4KB/s   00:01    
readmissions.csv                                                                                                                      100%   19MB   1.5MB/s   00:13    
surveys_responses.csv                                                                                                                 100% 1317KB   1.3MB/s   00:00    
localhost:W205 NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205
localhost:W205 NatarajanShankar$ !vi
vi effective_care.csv 
localhost:W205 NatarajanShankar$ cd week4
localhost:week4 NatarajanShankar$ vi hospitals.csv
localhost:week4 NatarajanShankar$ fgrep "ACS Participation" *.csv
Measures.csv:"ACS Participation data","ACS_REGISTRY","3Q2013","2013-07-01 00:00:00","2Q2014","2014-06-30 00:00:00"
localhost:week4 NatarajanShankar$ vi Measures.csv
localhost:week4 NatarajanShankar$ pwd
/Users/NatarajanShankar/UC_Berkeley/Term2/W205/week4
localhost:week4 NatarajanShankar$ vi log10042016.txt
localhost:week4 NatarajanShankar$ 
